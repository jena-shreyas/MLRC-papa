{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jena-shreyas/MLRC-papa/blob/main/PAPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5a2omcHjOzM",
        "outputId": "a02f5749-2fb3-4884-ea86-371f9c6b7342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬ Downloading https://github.com/jaimergp/miniforge/releases/latest/download/Mambaforge-colab-Linux-x86_64.sh...\n",
            "ğŸ“¦ Installing...\n",
            "ğŸ“Œ Adjusting configuration...\n",
            "ğŸ©¹ Patching environment...\n",
            "â² Done in 0:00:13\n",
            "ğŸ” Restarting kernel...\n"
          ]
        }
      ],
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lsb_release -a\n",
        "!python --version"
      ],
      "metadata": {
        "id": "6WqaOofaficr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "conda create --name env python=3.8 -y\n",
        "source activate env\n",
        "conda install pytorch -c pytorch-lts -c nvidia\n",
        "\n",
        "python -m pip install -U pip\n",
        "pip install --upgrade setuptools\n",
        "\n",
        "pip install transformers==4.18.0\n",
        "\n",
        "pip install datasets accelerate scipy scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLtrMTS-jfnF",
        "outputId": "da3f7461-dcb7-4506-c466-395a6ba8d88d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "  current version: 22.9.0\n",
            "  latest version: 22.11.1\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/env\n",
            "\n",
            "  added / updated specs:\n",
            "    - python=3.8\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    ca-certificates-2022.12.7  |       ha878542_0         143 KB  conda-forge\n",
            "    setuptools-65.6.3          |     pyhd8ed1ab_0         619 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         761 KB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge None\n",
            "  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu None\n",
            "  bzip2              conda-forge/linux-64::bzip2-1.0.8-h7f98852_4 None\n",
            "  ca-certificates    conda-forge/linux-64::ca-certificates-2022.12.7-ha878542_0 None\n",
            "  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.39-hcc3a1bd_1 None\n",
            "  libffi             conda-forge/linux-64::libffi-3.4.2-h7f98852_5 None\n",
            "  libgcc-ng          conda-forge/linux-64::libgcc-ng-12.2.0-h65d4601_19 None\n",
            "  libgomp            conda-forge/linux-64::libgomp-12.2.0-h65d4601_19 None\n",
            "  libnsl             conda-forge/linux-64::libnsl-2.0.0-h7f98852_0 None\n",
            "  libsqlite          conda-forge/linux-64::libsqlite-3.40.0-h753d276_0 None\n",
            "  libuuid            conda-forge/linux-64::libuuid-2.32.1-h7f98852_1000 None\n",
            "  libzlib            conda-forge/linux-64::libzlib-1.2.13-h166bdaf_4 None\n",
            "  ncurses            conda-forge/linux-64::ncurses-6.3-h27087fc_1 None\n",
            "  openssl            conda-forge/linux-64::openssl-3.0.7-h0b41bf4_1 None\n",
            "  pip                conda-forge/noarch::pip-22.3.1-pyhd8ed1ab_0 None\n",
            "  python             conda-forge/linux-64::python-3.8.15-h4a9ceb5_0_cpython None\n",
            "  readline           conda-forge/linux-64::readline-8.1.2-h0f457ee_0 None\n",
            "  setuptools         conda-forge/noarch::setuptools-65.6.3-pyhd8ed1ab_0 None\n",
            "  tk                 conda-forge/linux-64::tk-8.6.12-h27826a3_0 None\n",
            "  wheel              conda-forge/noarch::wheel-0.38.4-pyhd8ed1ab_0 None\n",
            "  xz                 conda-forge/linux-64::xz-5.2.6-h166bdaf_0 None\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "ca-certificates-2022 | 143 KB    | : 100% 1.0/1 [00:00<00:00,  7.28it/s]                \n",
            "setuptools-65.6.3    | 619 KB    | : 100% 1.0/1 [00:00<00:00,  6.66it/s]\n",
            "Preparing transaction: / \b\b- \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate env\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n",
            "Retrieving notices: ...working... done\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "  current version: 22.9.0\n",
            "  latest version: 22.11.1\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/env\n",
            "\n",
            "  added / updated specs:\n",
            "    - pytorch\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    _openmp_mutex-4.5          |       2_kmp_llvm           6 KB  conda-forge\n",
            "    blas-2.106                 |              mkl          12 KB  conda-forge\n",
            "    cudatoolkit-11.1.74        |       h6bb024c_0        1.19 GB  nvidia\n",
            "    libblas-3.9.0              |            6_mkl          11 KB  conda-forge\n",
            "    libcblas-3.9.0             |            6_mkl          11 KB  conda-forge\n",
            "    libgfortran-ng-12.2.0      |      h69a702a_19          22 KB  conda-forge\n",
            "    libgfortran5-12.2.0        |      h337968e_19         1.8 MB  conda-forge\n",
            "    liblapack-3.9.0            |            6_mkl          11 KB  conda-forge\n",
            "    liblapacke-3.9.0           |            6_mkl          11 KB  conda-forge\n",
            "    libuv-1.44.2               |       h166bdaf_0         1.0 MB  conda-forge\n",
            "    llvm-openmp-15.0.6         |       he0ac6c6_0         3.3 MB  conda-forge\n",
            "    mkl-2020.4                 |     h726a3e6_304       215.6 MB  conda-forge\n",
            "    ninja-1.11.0               |       h924138e_0         2.8 MB  conda-forge\n",
            "    numpy-1.24.1               |   py38hab0fcb9_0         6.3 MB  conda-forge\n",
            "    pytorch-1.8.2              |py3.8_cuda11.1_cudnn8.0.5_0        1.27 GB  pytorch-lts\n",
            "    typing_extensions-4.4.0    |     pyha770c72_0          29 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        2.68 GB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  blas               conda-forge/linux-64::blas-2.106-mkl None\n",
            "  cudatoolkit        nvidia/linux-64::cudatoolkit-11.1.74-h6bb024c_0 None\n",
            "  libblas            conda-forge/linux-64::libblas-3.9.0-6_mkl None\n",
            "  libcblas           conda-forge/linux-64::libcblas-3.9.0-6_mkl None\n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-12.2.0-h69a702a_19 None\n",
            "  libgfortran5       conda-forge/linux-64::libgfortran5-12.2.0-h337968e_19 None\n",
            "  liblapack          conda-forge/linux-64::liblapack-3.9.0-6_mkl None\n",
            "  liblapacke         conda-forge/linux-64::liblapacke-3.9.0-6_mkl None\n",
            "  libstdcxx-ng       conda-forge/linux-64::libstdcxx-ng-12.2.0-h46fd767_19 None\n",
            "  libuv              conda-forge/linux-64::libuv-1.44.2-h166bdaf_0 None\n",
            "  llvm-openmp        conda-forge/linux-64::llvm-openmp-15.0.6-he0ac6c6_0 None\n",
            "  mkl                conda-forge/linux-64::mkl-2020.4-h726a3e6_304 None\n",
            "  ninja              conda-forge/linux-64::ninja-1.11.0-h924138e_0 None\n",
            "  numpy              conda-forge/linux-64::numpy-1.24.1-py38hab0fcb9_0 None\n",
            "  python_abi         conda-forge/linux-64::python_abi-3.8-3_cp38 None\n",
            "  pytorch            pytorch-lts/linux-64::pytorch-1.8.2-py3.8_cuda11.1_cudnn8.0.5_0 None\n",
            "  typing_extensions  conda-forge/noarch::typing_extensions-4.4.0-pyha770c72_0 None\n",
            "\n",
            "The following packages will be DOWNGRADED:\n",
            "\n",
            "  _openmp_mutex                                   4.5-2_gnu --> 4.5-2_kmp_llvm None\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "libgfortran-ng-12.2. | 22 KB     | : 100% 1.0/1 [00:00<00:00,  7.12it/s]               \n",
            "liblapacke-3.9.0     | 11 KB     | : 100% 1.0/1 [00:00<00:00,  5.21it/s]\n",
            "numpy-1.24.1         | 6.3 MB    | : 100% 1.0/1 [00:00<00:00,  1.16it/s]\n",
            "mkl-2020.4           | 215.6 MB  | : 100% 1.0/1 [00:39<00:00, 39.37s/it]               \n",
            "libgfortran5-12.2.0  | 1.8 MB    | : 100% 1.0/1 [00:00<00:00,  3.14it/s]\n",
            "libuv-1.44.2         | 1.0 MB    | : 100% 1.0/1 [00:00<00:00,  5.71it/s]\n",
            "_openmp_mutex-4.5    | 6 KB      | : 100% 1.0/1 [00:00<00:00, 21.65it/s]\n",
            "cudatoolkit-11.1.74  | 1.19 GB   | : 100% 1.0/1 [02:27<00:00, 147.01s/it]              \n",
            "llvm-openmp-15.0.6   | 3.3 MB    | : 100% 1.0/1 [00:00<00:00,  3.74it/s]\n",
            "blas-2.106           | 12 KB     | : 100% 1.0/1 [00:00<00:00,  4.90it/s]\n",
            "pytorch-1.8.2        | 1.27 GB   | : 100% 1.0/1 [03:03<00:00, 183.30s/it]\n",
            "liblapack-3.9.0      | 11 KB     | : 100% 1.0/1 [00:00<00:00,  4.85it/s]\n",
            "libblas-3.9.0        | 11 KB     | : 100% 1.0/1 [00:00<00:00,  5.77it/s]\n",
            "libcblas-3.9.0       | 11 KB     | : 100% 1.0/1 [00:00<00:00,  4.65it/s]\n",
            "ninja-1.11.0         | 2.8 MB    | : 100% 1.0/1 [00:00<00:00,  2.43it/s]\n",
            "typing_extensions-4. | 29 KB     | : 100% 1.0/1 [00:00<00:00, 23.95it/s]\n",
            "Preparing transaction: \\ \b\b| \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n",
            "\n",
            "\b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Retrieving notices: ...working... done\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/envs/env/lib/python3.8/site-packages (22.3.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: setuptools in /usr/local/envs/env/lib/python3.8/site-packages (65.6.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.18.0\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting regex!=2019.12.17\n",
            "  Downloading regex-2022.10.31-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m772.3/772.3 kB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging>=20.0\n",
            "  Downloading packaging-22.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/envs/env/lib/python3.8/site-packages (from transformers==4.18.0) (1.24.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting requests\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm>=4.27\n",
            "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m701.2/701.2 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m182.4/182.4 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/envs/env/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0) (4.4.0)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<3,>=2\n",
            "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting six\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting click\n",
            "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting joblib\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=1aca58f283c461704318ae57354c594bf35464d31d83e65d24a0df1beabd7eb4\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/a3/ff/01dc060d7fc51176b3ce7cf1561466a12e658164b594747547\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, urllib3, tqdm, six, regex, pyyaml, packaging, joblib, idna, filelock, click, charset-normalizer, certifi, sacremoses, requests, huggingface-hub, transformers\n",
            "Successfully installed certifi-2022.12.7 charset-normalizer-2.1.1 click-8.1.3 filelock-3.9.0 huggingface-hub-0.11.1 idna-3.4 joblib-1.2.0 packaging-22.0 pyyaml-6.0 regex-2022.10.31 requests-2.28.1 sacremoses-0.0.53 six-1.16.0 tokenizers-0.12.1 tqdm-4.64.1 transformers-4.18.0 urllib3-1.26.13\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.8.0-py3-none-any.whl (452 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m452.9/452.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.15.0-py3-none-any.whl (191 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m191.5/191.5 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy\n",
            "  Downloading scipy-1.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn\n",
            "  Downloading scikit_learn-1.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/envs/env/lib/python3.8/site-packages (from datasets) (1.24.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/envs/env/lib/python3.8/site-packages (from datasets) (0.11.1)\n",
            "Collecting pyarrow>=6.0.0\n",
            "  Downloading pyarrow-10.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m36.0/36.0 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas\n",
            "  Downloading pandas-1.5.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/envs/env/lib/python3.8/site-packages (from datasets) (2.28.1)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fsspec[http]>=2021.11.1\n",
            "  Downloading fsspec-2022.11.0-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.5/139.5 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/envs/env/lib/python3.8/site-packages (from datasets) (6.0)\n",
            "Collecting dill<0.3.7\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/envs/env/lib/python3.8/site-packages (from datasets) (4.64.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m213.0/213.0 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/envs/env/lib/python3.8/site-packages (from datasets) (22.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/envs/env/lib/python3.8/site-packages (from accelerate) (1.8.2)\n",
            "Collecting psutil\n",
            "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m280.2/280.2 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /usr/local/envs/env/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/envs/env/lib/python3.8/site-packages (from aiohttp->datasets) (2.1.1)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m262.1/262.1 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting attrs>=17.3.0\n",
            "  Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m161.3/161.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/envs/env/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/envs/env/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/envs/env/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/envs/env/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/envs/env/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Collecting pytz>=2020.1\n",
            "  Downloading pytz-2022.7-py2.py3-none-any.whl (499 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m499.4/499.4 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dateutil>=2.8.1\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/envs/env/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: pytz, xxhash, threadpoolctl, scipy, python-dateutil, pyarrow, psutil, multidict, fsspec, frozenlist, dill, attrs, async-timeout, yarl, scikit-learn, responses, pandas, multiprocess, aiosignal, accelerate, aiohttp, datasets\n",
            "Successfully installed accelerate-0.15.0 aiohttp-3.8.3 aiosignal-1.3.1 async-timeout-4.0.2 attrs-22.2.0 datasets-2.8.0 dill-0.3.6 frozenlist-1.3.3 fsspec-2022.11.0 multidict-6.0.4 multiprocess-0.70.14 pandas-1.5.2 psutil-5.9.4 pyarrow-10.0.1 python-dateutil-2.8.2 pytz-2022.7 responses-0.18.0 scikit-learn-1.2.0 scipy-1.10.0 threadpoolctl-3.1.0 xxhash-3.2.0 yarl-1.8.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "source activate env\n",
        "git clone https://github.com/schwartz-lab-NLP/papa.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8HUnX1pniek",
        "outputId": "8f20ec7c-2ae5-4122-b957-cb5fac441f85"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'papa'...\n",
            "remote: Enumerating objects: 120, done.\u001b[K\n",
            "remote: Counting objects: 100% (120/120), done.\u001b[K\n",
            "remote: Compressing objects: 100% (96/96), done.\u001b[K\n",
            "remote: Total 120 (delta 40), reused 1 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (120/120), 138.92 KiB | 6.31 MiB/s, done.\n",
            "Resolving deltas: 100% (40/40), done.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%shell\n",
        "# source activate env\n",
        "# pip uninstall transformers -y\n",
        "# pip install transformers==4.18.0\n",
        "# pip show transformers"
      ],
      "metadata": {
        "id": "_LWhBX2jECGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy local transformers content to installed transformers location\n",
        "!cp -R papa/transformers/src/transformers/* /usr/local/envs/env/lib/python3.8/site-packages/transformers/"
      ],
      "metadata": {
        "id": "WOu5NBYqnDS_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "source activate env\n",
        "cd papa/transformers/papa_scripts\n",
        "\n",
        "MODEL=bert-base-uncased\n",
        "TASK=cola\n",
        "\n",
        "python3 run_papa_glue_avgs_creator.py --model_name_or_path ${MODEL}  --task_name ${TASK}  --max_length 64    --per_device_train_batch_size 8   --output_dir ../../constant-matrices/ --cache_dir ../../cache/ --use_papa_preprocess true  --pad_to_max_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMBMB-MZn_iz",
        "outputId": "02a5219a-bed9-4b81-baf7-2a62fc8c7ac7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01/07/2023 19:31:22 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "Mixed precision type: no\n",
            "\n",
            "Downloading builder script: 100% 28.8k/28.8k [00:00<00:00, 1.65MB/s]\n",
            "Downloading metadata: 100% 28.7k/28.7k [00:00<00:00, 1.55MB/s]\n",
            "Downloading readme: 100% 27.8k/27.8k [00:00<00:00, 1.51MB/s]\n",
            "Downloading and preparing dataset glue/cola to /content/papa/transformers/papa_scripts/../../cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "Downloading data: 100% 377k/377k [00:00<00:00, 1.14MB/s]\n",
            "Dataset glue downloaded and prepared to /content/papa/transformers/papa_scripts/../../cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 512.10it/s]\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp2vlalt3c\n",
            "Downloading: 100% 570/570 [00:00<00:00, 487kB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_seq_length\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sort_calculating\": null,\n",
            "  \"static_heads\": null,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"use_freeze_extract_pooler\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpln7k58kh\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 33.3kB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_seq_length\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sort_calculating\": null,\n",
            "  \"static_heads\": null,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"use_freeze_extract_pooler\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp1x6o5_22\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 4.09MB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpslzqak5u\n",
            "Downloading: 100% 455k/455k [00:00<00:00, 5.02MB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_seq_length\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sort_calculating\": null,\n",
            "  \"static_heads\": null,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"use_freeze_extract_pooler\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /content/papa/cache/tmpn3d8xw97\n",
            "Downloading: 100% 420M/420M [00:05<00:00, 79.9MB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at ../../cache/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "creating metadata file for ../../cache/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at ../../cache/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset: 100% 9/9 [00:00<00:00, 14.51ba/s]\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00, 31.25ba/s]\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00, 31.73ba/s]\n",
            "01/07/2023 19:31:35 - INFO - __main__ - Sample 4864 of the training set: {'input_ids': [101, 2002, 2273, 5732, 1996, 10818, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'labels': 1}.\n",
            "01/07/2023 19:31:35 - INFO - __main__ - Sample 729 of the training set: {'input_ids': [101, 2184, 6053, 2001, 12781, 2011, 1996, 7427, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'labels': 0}.\n",
            "01/07/2023 19:31:35 - INFO - __main__ - Sample 1103 of the training set: {'input_ids': [101, 2198, 2003, 12283, 2084, 3021, 2003, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'labels': 1}.\n",
            "/usr/local/envs/env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "run_papa_glue_avgs_creator.py:452: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"glue\", args.task_name)\n",
            "Downloading builder script: 5.76kB [00:00, 7.16MB/s]       \n",
            "01/07/2023 19:31:38 - INFO - __main__ - ***** Running training *****\n",
            "01/07/2023 19:31:38 - INFO - __main__ -   Num examples = 8551\n",
            "01/07/2023 19:31:38 - INFO - __main__ -   Num Epochs = 3\n",
            "01/07/2023 19:31:38 - INFO - __main__ -   Instantaneous batch size per device = 8\n",
            "01/07/2023 19:31:38 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "01/07/2023 19:31:38 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
            "01/07/2023 19:31:38 - INFO - __main__ -   Total optimization steps = 3207\n",
            "  0% 0/3207 [00:34<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%shell\n",
        "source activate env\n",
        "cd papa/transformers/papa_scripts\n",
        "\n",
        "MODEL=bert-base-uncased\n",
        "TASK=cola\n",
        "\n",
        "python3 run_papa_glue.py --model_name_or_path ${MODEL} --task_name ${TASK} --do_eval --max_seq_length 64 --per_device_train_batch_size 16 --per_device_eval_batch_size 16 --output_dir ../../sorted-heads/ --cache_dir ../../cache/  --do_train --num_train_epochs 15.0 --learning_rate 2e-5 --lr_scheduler_type constant --disable_tqdm true --evaluation_strategy epoch --save_strategy no --use_papa_preprocess --use_freeze_extract_pooler true --static_heads_dir ../../constant-matrices/  --save_total_limit 0 --sort_calculating True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3Dmx5bsl1o0",
        "outputId": "95ec263b-0d51-4139-dc4c-213d31e12721"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01/07/2023 19:38:59 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "01/07/2023 19:38:59 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=True,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=../../sorted-heads/runs/Jan07_19-38-59_6dbe8a223432,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.CONSTANT,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=15.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=../../sorted-heads/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=16,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=[],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=../../sorted-heads/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.NO,\n",
            "save_total_limit=0,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "01/07/2023 19:39:00 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "01/07/2023 19:39:00 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "01/07/2023 19:39:00 - INFO - datasets.info - Loading Dataset info from ../../cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "01/07/2023 19:39:00 - WARNING - datasets.builder - Found cached dataset glue (/content/papa/transformers/papa_scripts/../../cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "01/07/2023 19:39:00 - INFO - datasets.info - Loading Dataset info from /content/papa/transformers/papa_scripts/../../cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "100% 3/3 [00:00<00:00, 937.97it/s]\n",
            "[INFO|hub.py:583] 2023-01-07 19:39:00,579 >> https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /content/papa/cache/tmp5y91yuq4\n",
            "Downloading: 100% 570/570 [00:00<00:00, 796kB/s]\n",
            "[INFO|hub.py:587] 2023-01-07 19:39:00,697 >> storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at ../../cache/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|hub.py:595] 2023-01-07 19:39:00,697 >> creating metadata file for ../../cache/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:654] 2023-01-07 19:39:00,697 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at ../../cache/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:690] 2023-01-07 19:39:00,698 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_seq_length\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sort_calculating\": null,\n",
            "  \"static_heads\": null,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"use_freeze_extract_pooler\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|hub.py:583] 2023-01-07 19:39:00,809 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /content/papa/cache/tmppache1l4\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 40.6kB/s]\n",
            "[INFO|hub.py:587] 2023-01-07 19:39:00,922 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at ../../cache/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|hub.py:595] 2023-01-07 19:39:00,922 >> creating metadata file for ../../cache/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|configuration_utils.py:654] 2023-01-07 19:39:01,035 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at ../../cache/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:690] 2023-01-07 19:39:01,036 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_seq_length\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sort_calculating\": null,\n",
            "  \"static_heads\": null,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"use_freeze_extract_pooler\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|hub.py:583] 2023-01-07 19:39:01,260 >> https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /content/papa/cache/tmp_pqkivp5\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 3.12MB/s]\n",
            "[INFO|hub.py:587] 2023-01-07 19:39:01,452 >> storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at ../../cache/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|hub.py:595] 2023-01-07 19:39:01,453 >> creating metadata file for ../../cache/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|hub.py:583] 2023-01-07 19:39:01,569 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /content/papa/cache/tmp48jmxz7a\n",
            "Downloading: 100% 455k/455k [00:00<00:00, 6.18MB/s]\n",
            "[INFO|hub.py:587] 2023-01-07 19:39:01,791 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at ../../cache/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|hub.py:595] 2023-01-07 19:39:01,791 >> creating metadata file for ../../cache/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 19:39:02,136 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at ../../cache/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 19:39:02,136 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at ../../cache/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 19:39:02,136 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 19:39:02,136 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 19:39:02,136 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at ../../cache/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|configuration_utils.py:654] 2023-01-07 19:39:02,249 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at ../../cache/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:690] 2023-01-07 19:39:02,250 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_seq_length\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sort_calculating\": null,\n",
            "  \"static_heads\": null,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"use_freeze_extract_pooler\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1430] 2023-01-07 19:39:02,414 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at ../../cache/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[WARNING|modeling_utils.py:1688] 2023-01-07 19:39:04,093 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'bert.pooler.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1699] 2023-01-07 19:39:04,093 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.3.attention.self.lambdas', 'classifier.weight', 'bert.encoder.layer.7.attention.self.lambdas', 'bert.encoder.layer.8.attention.self.static_heads_content', 'bert.encoder.layer.9.attention.self.lambdas', 'bert.encoder.layer.11.attention.self.lambdas', 'bert.encoder.layer.11.attention.self.static_heads_content', 'bert.encoder.layer.9.attention.self.static_heads_content', 'bert.encoder.layer.6.attention.self.lambdas', 'bert.encoder.layer.1.attention.self.lambdas', 'bert.encoder.layer.4.attention.self.lambdas', 'bert.pooler.dense_across_layers.bias', 'bert.pooler.dense_across_layers.weight', 'bert.encoder.layer.3.attention.self.static_heads_content', 'bert.encoder.layer.0.attention.self.lambdas', 'bert.encoder.layer.7.attention.self.static_heads_content', 'bert.encoder.layer.4.attention.self.static_heads_content', 'classifier.bias', 'bert.encoder.layer.6.attention.self.static_heads_content', 'bert.encoder.layer.5.attention.self.static_heads_content', 'bert.encoder.layer.8.attention.self.lambdas', 'bert.encoder.layer.2.attention.self.lambdas', 'bert.encoder.layer.10.attention.self.lambdas', 'bert.encoder.layer.1.attention.self.static_heads_content', 'bert.encoder.layer.2.attention.self.static_heads_content', 'bert.encoder.layer.10.attention.self.static_heads_content', 'bert.pooler.weights_per_layer', 'bert.encoder.layer.0.attention.self.static_heads_content', 'bert.encoder.layer.5.attention.self.lambdas']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Not frozen param:  bert.encoder.layer.0.attention.self.lambdas\n",
            "Not frozen param:  bert.encoder.layer.1.attention.self.lambdas\n",
            "Not frozen param:  bert.encoder.layer.2.attention.self.lambdas\n",
            "Not frozen param:  bert.encoder.layer.3.attention.self.lambdas\n",
            "Not frozen param:  bert.encoder.layer.4.attention.self.lambdas\n",
            "Not frozen param:  bert.encoder.layer.5.attention.self.lambdas\n",
            "Not frozen param:  bert.encoder.layer.6.attention.self.lambdas\n",
            "Not frozen param:  bert.encoder.layer.7.attention.self.lambdas\n",
            "Not frozen param:  bert.encoder.layer.8.attention.self.lambdas\n",
            "Not frozen param:  bert.encoder.layer.9.attention.self.lambdas\n",
            "Not frozen param:  bert.encoder.layer.10.attention.self.lambdas\n",
            "Not frozen param:  bert.encoder.layer.11.attention.self.lambdas\n",
            "Not frozen param:  bert.pooler.weights_per_layer\n",
            "Not frozen param:  bert.pooler.dense_across_layers.weight\n",
            "Not frozen param:  bert.pooler.dense_across_layers.bias\n",
            "Not frozen param:  classifier.weight\n",
            "Not frozen param:  classifier.bias\n",
            "Running tokenizer on dataset:   0% 0/9 [00:00<?, ?ba/s]01/07/2023 19:39:04 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/papa/cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-89b22c6772fe5dc5.arrow\n",
            "Running tokenizer on dataset: 100% 9/9 [00:00<00:00, 12.90ba/s]\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]01/07/2023 19:39:04 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/papa/cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-101b8c27ae391d4f.arrow\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00, 25.59ba/s]\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]01/07/2023 19:39:04 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/papa/cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2d744ddb657954a1.arrow\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00, 27.10ba/s]\n",
            "01/07/2023 19:39:05 - INFO - __main__ - Sample 1824 of the training set: {'sentence': 'I acknowledged that my father, he was tight as an owl.', 'label': 0, 'idx': 1824, 'input_ids': [101, 1045, 8969, 2008, 2026, 2269, 1010, 2002, 2001, 4389, 2004, 2019, 13547, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'old_input_ids': [101, 1045, 8969, 2008, 2026, 2269, 1010, 2002, 2001, 4389, 2004, 2019, 13547, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'old_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "01/07/2023 19:39:05 - INFO - __main__ - Sample 409 of the training set: {'sentence': 'For him to do that would be a mistake.', 'label': 1, 'idx': 409, 'input_ids': [101, 2005, 2032, 2000, 2079, 2008, 2052, 2022, 1037, 6707, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'old_input_ids': [101, 2005, 2032, 2000, 2079, 2008, 2052, 2022, 1037, 6707, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'old_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "01/07/2023 19:39:05 - INFO - __main__ - Sample 4506 of the training set: {'sentence': 'Mary sang a song, but Lee never did.', 'label': 1, 'idx': 4506, 'input_ids': [101, 2984, 6369, 1037, 2299, 1010, 2021, 3389, 2196, 2106, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'old_input_ids': [101, 2984, 6369, 1037, 2299, 1010, 2021, 3389, 2196, 2106, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'old_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "run_papa_glue.py:567: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"glue\", data_args.task_name)\n",
            "[INFO|trainer.py:566] 2023-01-07 19:39:07,909 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, old_attention_mask, sentence, idx. If old_input_ids, old_attention_mask, sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/envs/env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1290] 2023-01-07 19:39:07,918 >> ***** Running training *****\n",
            "[INFO|trainer.py:1291] 2023-01-07 19:39:07,918 >>   Num examples = 8551\n",
            "[INFO|trainer.py:1292] 2023-01-07 19:39:07,918 >>   Num Epochs = 15\n",
            "[INFO|trainer.py:1293] 2023-01-07 19:39:07,918 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1294] 2023-01-07 19:39:07,918 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1295] 2023-01-07 19:39:07,918 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1296] 2023-01-07 19:39:07,918 >>   Total optimization steps = 8025\n",
            "{'loss': 0.5746, 'learning_rate': 2e-05, 'epoch': 0.93}\n",
            "[INFO|trainer.py:566] 2023-01-07 19:40:08,755 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, old_attention_mask, sentence, idx. If old_input_ids, old_attention_mask, sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 19:40:08,757 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 19:40:08,757 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 19:40:08,757 >>   Batch size = 16\n",
            "01/07/2023 19:40:12 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5603216886520386, 'eval_matthews_correlation': 0.22781131848083777, 'eval_runtime': 3.9536, 'eval_samples_per_second': 263.807, 'eval_steps_per_second': 16.693, 'epoch': 1.0}\n",
            "{'loss': 0.519, 'learning_rate': 2e-05, 'epoch': 1.87}\n",
            "[INFO|trainer.py:566] 2023-01-07 19:41:15,891 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, old_attention_mask, sentence, idx. If old_input_ids, old_attention_mask, sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 19:41:15,893 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 19:41:15,893 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 19:41:15,893 >>   Batch size = 16\n",
            "01/07/2023 19:41:19 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5284054279327393, 'eval_matthews_correlation': 0.3046308372846143, 'eval_runtime': 4.0259, 'eval_samples_per_second': 259.072, 'eval_steps_per_second': 16.394, 'epoch': 2.0}\n",
            "{'loss': 0.505, 'learning_rate': 2e-05, 'epoch': 2.8}\n",
            "[INFO|trainer.py:566] 2023-01-07 19:42:24,206 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, old_attention_mask, sentence, idx. If old_input_ids, old_attention_mask, sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 19:42:24,208 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 19:42:24,208 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 19:42:24,208 >>   Batch size = 16\n",
            "01/07/2023 19:42:28 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5299597978591919, 'eval_matthews_correlation': 0.3222180119516509, 'eval_runtime': 4.1358, 'eval_samples_per_second': 252.19, 'eval_steps_per_second': 15.958, 'epoch': 3.0}\n",
            "{'loss': 0.4882, 'learning_rate': 2e-05, 'epoch': 3.74}\n",
            "[INFO|trainer.py:566] 2023-01-07 19:43:33,466 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, old_attention_mask, sentence, idx. If old_input_ids, old_attention_mask, sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 19:43:33,468 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 19:43:33,468 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 19:43:33,469 >>   Batch size = 16\n",
            "01/07/2023 19:43:37 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5088865756988525, 'eval_matthews_correlation': 0.3702568889845357, 'eval_runtime': 4.163, 'eval_samples_per_second': 250.542, 'eval_steps_per_second': 15.854, 'epoch': 4.0}\n",
            "{'loss': 0.4822, 'learning_rate': 2e-05, 'epoch': 4.67}\n",
            "[INFO|trainer.py:566] 2023-01-07 19:44:43,109 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, old_attention_mask, sentence, idx. If old_input_ids, old_attention_mask, sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 19:44:43,112 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 19:44:43,112 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 19:44:43,112 >>   Batch size = 16\n",
            "01/07/2023 19:44:47 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5019650459289551, 'eval_matthews_correlation': 0.36986609141954047, 'eval_runtime': 4.3532, 'eval_samples_per_second': 239.593, 'eval_steps_per_second': 15.161, 'epoch': 5.0}\n",
            "{'loss': 0.4737, 'learning_rate': 2e-05, 'epoch': 5.61}\n",
            "[INFO|trainer.py:566] 2023-01-07 19:45:52,759 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, old_attention_mask, sentence, idx. If old_input_ids, old_attention_mask, sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 19:45:52,761 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 19:45:52,761 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 19:45:52,761 >>   Batch size = 16\n",
            "01/07/2023 19:45:56 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5477095246315002, 'eval_matthews_correlation': 0.3818070077211945, 'eval_runtime': 4.1533, 'eval_samples_per_second': 251.126, 'eval_steps_per_second': 15.891, 'epoch': 6.0}\n",
            "{'loss': 0.4652, 'learning_rate': 2e-05, 'epoch': 6.54}\n",
            "[INFO|trainer.py:566] 2023-01-07 19:47:02,118 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, old_attention_mask, sentence, idx. If old_input_ids, old_attention_mask, sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 19:47:02,120 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 19:47:02,120 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 19:47:02,120 >>   Batch size = 16\n",
            "01/07/2023 19:47:06 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5359765887260437, 'eval_matthews_correlation': 0.39302533664823136, 'eval_runtime': 4.1727, 'eval_samples_per_second': 249.959, 'eval_steps_per_second': 15.817, 'epoch': 7.0}\n",
            "{'loss': 0.4498, 'learning_rate': 2e-05, 'epoch': 7.48}\n",
            "[INFO|trainer.py:566] 2023-01-07 19:48:11,819 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, old_attention_mask, sentence, idx. If old_input_ids, old_attention_mask, sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 19:48:11,822 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 19:48:11,822 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 19:48:11,822 >>   Batch size = 16\n",
            "01/07/2023 19:48:16 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5259395837783813, 'eval_matthews_correlation': 0.39216697445681503, 'eval_runtime': 4.3644, 'eval_samples_per_second': 238.978, 'eval_steps_per_second': 15.122, 'epoch': 8.0}\n",
            "{'loss': 0.4527, 'learning_rate': 2e-05, 'epoch': 8.41}\n",
            "[INFO|trainer.py:566] 2023-01-07 19:49:21,436 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, old_attention_mask, sentence, idx. If old_input_ids, old_attention_mask, sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 19:49:21,438 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 19:49:21,438 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 19:49:21,438 >>   Batch size = 16\n",
            "01/07/2023 19:49:25 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5107746720314026, 'eval_matthews_correlation': 0.42425075806204515, 'eval_runtime': 4.1511, 'eval_samples_per_second': 251.257, 'eval_steps_per_second': 15.899, 'epoch': 9.0}\n",
            "{'loss': 0.4379, 'learning_rate': 2e-05, 'epoch': 9.35}\n",
            "[INFO|trainer.py:566] 2023-01-07 19:50:30,774 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, old_attention_mask, sentence, idx. If old_input_ids, old_attention_mask, sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 19:50:30,776 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 19:50:30,776 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 19:50:30,776 >>   Batch size = 16\n",
            "01/07/2023 19:50:34 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5219295620918274, 'eval_matthews_correlation': 0.4063609883970663, 'eval_runtime': 4.1512, 'eval_samples_per_second': 251.256, 'eval_steps_per_second': 15.899, 'epoch': 10.0}\n",
            "{'loss': 0.4356, 'learning_rate': 2e-05, 'epoch': 10.28}\n",
            "[INFO|trainer.py:566] 2023-01-07 19:51:40,286 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, old_attention_mask, sentence, idx. If old_input_ids, old_attention_mask, sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 19:51:40,288 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 19:51:40,288 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 19:51:40,288 >>   Batch size = 16\n",
            "01/07/2023 19:51:44 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.4964248538017273, 'eval_matthews_correlation': 0.44110773079251625, 'eval_runtime': 4.3401, 'eval_samples_per_second': 240.317, 'eval_steps_per_second': 15.207, 'epoch': 11.0}\n",
            "{'loss': 0.4436, 'learning_rate': 2e-05, 'epoch': 11.21}\n",
            "[INFO|trainer.py:566] 2023-01-07 19:52:50,032 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, old_attention_mask, sentence, idx. If old_input_ids, old_attention_mask, sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 19:52:50,034 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 19:52:50,034 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 19:52:50,034 >>   Batch size = 16\n",
            "01/07/2023 19:52:54 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5220783352851868, 'eval_matthews_correlation': 0.4302323404760539, 'eval_runtime': 4.167, 'eval_samples_per_second': 250.302, 'eval_steps_per_second': 15.839, 'epoch': 12.0}\n",
            "{'loss': 0.4211, 'learning_rate': 2e-05, 'epoch': 12.15}\n",
            "[INFO|trainer.py:566] 2023-01-07 19:53:59,393 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, old_attention_mask, sentence, idx. If old_input_ids, old_attention_mask, sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 19:53:59,395 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 19:53:59,395 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 19:53:59,395 >>   Batch size = 16\n",
            "01/07/2023 19:54:03 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.4957847595214844, 'eval_matthews_correlation': 0.44229349485124575, 'eval_runtime': 4.1562, 'eval_samples_per_second': 250.952, 'eval_steps_per_second': 15.88, 'epoch': 13.0}\n",
            "{'loss': 0.4309, 'learning_rate': 2e-05, 'epoch': 13.08}\n",
            "[INFO|trainer.py:566] 2023-01-07 19:55:08,988 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, old_attention_mask, sentence, idx. If old_input_ids, old_attention_mask, sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 19:55:08,991 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 19:55:08,991 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 19:55:08,991 >>   Batch size = 16\n",
            "01/07/2023 19:55:13 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5206599831581116, 'eval_matthews_correlation': 0.4304260866765428, 'eval_runtime': 4.378, 'eval_samples_per_second': 238.239, 'eval_steps_per_second': 15.076, 'epoch': 14.0}\n",
            "{'loss': 0.4245, 'learning_rate': 2e-05, 'epoch': 14.02}\n",
            "{'loss': 0.4157, 'learning_rate': 2e-05, 'epoch': 14.95}\n",
            "[INFO|trainer.py:566] 2023-01-07 19:56:18,793 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, old_attention_mask, sentence, idx. If old_input_ids, old_attention_mask, sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 19:56:18,795 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 19:56:18,795 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 19:56:18,795 >>   Batch size = 16\n",
            "01/07/2023 19:56:22 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5010701417922974, 'eval_matthews_correlation': 0.41820216419095174, 'eval_runtime': 4.1687, 'eval_samples_per_second': 250.197, 'eval_steps_per_second': 15.832, 'epoch': 15.0}\n",
            "[INFO|trainer.py:1530] 2023-01-07 19:56:22,964 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1035.0456, 'train_samples_per_second': 123.922, 'train_steps_per_second': 7.753, 'train_loss': 0.46357236083794234, 'epoch': 15.0}\n",
            "[INFO|configuration_utils.py:441] 2023-01-07 19:56:22,964 >> Configuration saved in ../../sorted-heads/config.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       15.0\n",
            "  train_loss               =     0.4636\n",
            "  train_runtime            = 0:17:15.04\n",
            "  train_samples            =       8551\n",
            "  train_samples_per_second =    123.922\n",
            "  train_steps_per_second   =      7.753\n",
            "01/07/2023 19:56:22 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:566] 2023-01-07 19:56:22,968 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, old_attention_mask, sentence, idx. If old_input_ids, old_attention_mask, sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 19:56:22,970 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 19:56:22,970 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 19:56:22,970 >>   Batch size = 16\n",
            "01/07/2023 19:56:27 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5010701417922974, 'eval_matthews_correlation': 0.41820216419095174, 'eval_runtime': 4.1618, 'eval_samples_per_second': 250.612, 'eval_steps_per_second': 15.858, 'epoch': 15.0}\n",
            "***** eval metrics *****\n",
            "  epoch                     =       15.0\n",
            "  eval_loss                 =     0.5011\n",
            "  eval_matthews_correlation =     0.4182\n",
            "  eval_runtime              = 0:00:04.16\n",
            "  eval_samples              =       1043\n",
            "  eval_samples_per_second   =    250.612\n",
            "  eval_steps_per_second     =     15.858\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "source activate env\n",
        "cd papa/transformers/papa_scripts\n",
        "\n",
        "MODEL=bert-base-uncased\n",
        "TASK=cola\n",
        "\n",
        "for static_heads_num in 0 72 126 135 144\n",
        " do\n",
        "python3 run_papa_glue.py --model_name_or_path ${MODEL} --task_name ${TASK} --do_eval --max_seq_length 64 --per_device_train_batch_size 16 --per_device_eval_batch_size 16 --output_dir ../../output/ --overwrite_output_dir --cache_dir ../../cache/ --do_train --num_train_epochs 15.0 --learning_rate 2e-5 --lr_scheduler_type constant --disable_tqdm true --evaluation_strategy epoch --save_strategy no --use_papa_preprocess --grad_for_classifier_only true --use_freeze_extract_pooler true --static_heads_dir ../../constant-matrices/ --static_heads_num ${static_heads_num} --save_total_limit 0 --sorting_heads_dir ../../sorted-heads/\n",
        "done"
      ],
      "metadata": {
        "id": "_PEzkile1Nfu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09a906df-0f46-465c-8ce2-7d652194581b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01/07/2023 20:02:05 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "01/07/2023 20:02:05 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=True,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=../../output/runs/Jan07_20-02-05_6dbe8a223432,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.CONSTANT,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=15.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=../../output/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=16,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=[],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=../../output/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.NO,\n",
            "save_total_limit=0,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "01/07/2023 20:02:06 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "01/07/2023 20:02:06 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "01/07/2023 20:02:06 - INFO - datasets.info - Loading Dataset info from ../../cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "01/07/2023 20:02:06 - WARNING - datasets.builder - Found cached dataset glue (/content/papa/transformers/papa_scripts/../../cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "01/07/2023 20:02:06 - INFO - datasets.info - Loading Dataset info from /content/papa/transformers/papa_scripts/../../cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "100% 3/3 [00:00<00:00, 444.39it/s]\n",
            "[INFO|configuration_utils.py:654] 2023-01-07 20:02:06,905 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at ../../cache/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:690] 2023-01-07 20:02:06,906 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_seq_length\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sort_calculating\": null,\n",
            "  \"static_heads\": null,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"use_freeze_extract_pooler\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2023-01-07 20:02:07,183 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at ../../cache/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:690] 2023-01-07 20:02:07,184 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_seq_length\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sort_calculating\": null,\n",
            "  \"static_heads\": null,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"use_freeze_extract_pooler\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 20:02:08,021 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at ../../cache/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 20:02:08,021 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at ../../cache/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 20:02:08,021 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 20:02:08,021 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 20:02:08,021 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at ../../cache/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|configuration_utils.py:654] 2023-01-07 20:02:08,159 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at ../../cache/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:690] 2023-01-07 20:02:08,160 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_seq_length\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sort_calculating\": null,\n",
            "  \"static_heads\": null,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"use_freeze_extract_pooler\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1430] 2023-01-07 20:02:08,391 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at ../../cache/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[WARNING|modeling_utils.py:1688] 2023-01-07 20:02:10,869 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1699] 2023-01-07 20:02:10,869 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'bert.pooler.weights_per_layer', 'bert.pooler.dense_across_layers.bias', 'classifier.bias', 'bert.pooler.dense_across_layers.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Not frozen param:  bert.pooler.weights_per_layer\n",
            "Not frozen param:  bert.pooler.dense_across_layers.weight\n",
            "Not frozen param:  bert.pooler.dense_across_layers.bias\n",
            "Not frozen param:  classifier.weight\n",
            "Not frozen param:  classifier.bias\n",
            "01/07/2023 20:02:10 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/papa/cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-89b22c6772fe5dc5.arrow\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]01/07/2023 20:02:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/papa/cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-9bb9ed57acfe3164.arrow\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00,  7.83ba/s]\n",
            "01/07/2023 20:02:11 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/papa/cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2d744ddb657954a1.arrow\n",
            "01/07/2023 20:02:11 - INFO - __main__ - Sample 1824 of the training set: {'sentence': 'I acknowledged that my father, he was tight as an owl.', 'label': 0, 'idx': 1824, 'input_ids': [101, 1045, 8969, 2008, 2026, 2269, 1010, 2002, 2001, 4389, 2004, 2019, 13547, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'old_input_ids': [101, 1045, 8969, 2008, 2026, 2269, 1010, 2002, 2001, 4389, 2004, 2019, 13547, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'old_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "01/07/2023 20:02:11 - INFO - __main__ - Sample 409 of the training set: {'sentence': 'For him to do that would be a mistake.', 'label': 1, 'idx': 409, 'input_ids': [101, 2005, 2032, 2000, 2079, 2008, 2052, 2022, 1037, 6707, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'old_input_ids': [101, 2005, 2032, 2000, 2079, 2008, 2052, 2022, 1037, 6707, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'old_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "01/07/2023 20:02:11 - INFO - __main__ - Sample 4506 of the training set: {'sentence': 'Mary sang a song, but Lee never did.', 'label': 1, 'idx': 4506, 'input_ids': [101, 2984, 6369, 1037, 2299, 1010, 2021, 3389, 2196, 2106, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'old_input_ids': [101, 2984, 6369, 1037, 2299, 1010, 2021, 3389, 2196, 2106, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'old_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "run_papa_glue.py:567: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"glue\", data_args.task_name)\n",
            "[INFO|trainer.py:566] 2023-01-07 20:02:15,032 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/envs/env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1290] 2023-01-07 20:02:15,044 >> ***** Running training *****\n",
            "[INFO|trainer.py:1291] 2023-01-07 20:02:15,044 >>   Num examples = 8551\n",
            "[INFO|trainer.py:1292] 2023-01-07 20:02:15,044 >>   Num Epochs = 15\n",
            "[INFO|trainer.py:1293] 2023-01-07 20:02:15,044 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1294] 2023-01-07 20:02:15,044 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1295] 2023-01-07 20:02:15,044 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1296] 2023-01-07 20:02:15,045 >>   Total optimization steps = 8025\n",
            "{'loss': 0.5716, 'learning_rate': 2e-05, 'epoch': 0.93}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:02:48,127 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:02:48,129 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:02:48,129 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:02:48,129 >>   Batch size = 16\n",
            "01/07/2023 20:02:51 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5569828152656555, 'eval_matthews_correlation': 0.23267650715203794, 'eval_runtime': 3.7135, 'eval_samples_per_second': 280.87, 'eval_steps_per_second': 17.773, 'epoch': 1.0}\n",
            "{'loss': 0.5098, 'learning_rate': 2e-05, 'epoch': 1.87}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:03:24,645 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:03:24,646 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:03:24,647 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:03:24,647 >>   Batch size = 16\n",
            "01/07/2023 20:03:28 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5243011116981506, 'eval_matthews_correlation': 0.35384164089639386, 'eval_runtime': 3.8034, 'eval_samples_per_second': 274.226, 'eval_steps_per_second': 17.353, 'epoch': 2.0}\n",
            "{'loss': 0.4942, 'learning_rate': 2e-05, 'epoch': 2.8}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:04:02,135 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:04:02,137 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:04:02,137 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:04:02,137 >>   Batch size = 16\n",
            "01/07/2023 20:04:06 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5313540697097778, 'eval_matthews_correlation': 0.33867454976929645, 'eval_runtime': 3.8772, 'eval_samples_per_second': 269.011, 'eval_steps_per_second': 17.023, 'epoch': 3.0}\n",
            "{'loss': 0.478, 'learning_rate': 2e-05, 'epoch': 3.74}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:04:40,244 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:04:40,246 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:04:40,246 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:04:40,246 >>   Batch size = 16\n",
            "01/07/2023 20:04:44 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5132347941398621, 'eval_matthews_correlation': 0.38293606916532946, 'eval_runtime': 3.9808, 'eval_samples_per_second': 262.007, 'eval_steps_per_second': 16.58, 'epoch': 4.0}\n",
            "{'loss': 0.471, 'learning_rate': 2e-05, 'epoch': 4.67}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:05:19,014 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:05:19,016 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:05:19,016 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:05:19,016 >>   Batch size = 16\n",
            "01/07/2023 20:05:23 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5073660016059875, 'eval_matthews_correlation': 0.3827349494084141, 'eval_runtime': 3.99, 'eval_samples_per_second': 261.403, 'eval_steps_per_second': 16.541, 'epoch': 5.0}\n",
            "{'loss': 0.4681, 'learning_rate': 2e-05, 'epoch': 5.61}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:05:58,652 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:05:58,654 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:05:58,654 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:05:58,654 >>   Batch size = 16\n",
            "01/07/2023 20:06:02 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5699759721755981, 'eval_matthews_correlation': 0.36689045725372754, 'eval_runtime': 3.997, 'eval_samples_per_second': 260.946, 'eval_steps_per_second': 16.512, 'epoch': 6.0}\n",
            "{'loss': 0.4564, 'learning_rate': 2e-05, 'epoch': 6.54}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:06:37,785 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:06:37,787 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:06:37,787 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:06:37,787 >>   Batch size = 16\n",
            "01/07/2023 20:06:41 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5437209606170654, 'eval_matthews_correlation': 0.3868598945304572, 'eval_runtime': 3.9792, 'eval_samples_per_second': 262.116, 'eval_steps_per_second': 16.586, 'epoch': 7.0}\n",
            "{'loss': 0.444, 'learning_rate': 2e-05, 'epoch': 7.48}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:07:16,930 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:07:16,932 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:07:16,932 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:07:16,932 >>   Batch size = 16\n",
            "01/07/2023 20:07:20 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5428090691566467, 'eval_matthews_correlation': 0.3899494680717728, 'eval_runtime': 3.9851, 'eval_samples_per_second': 261.723, 'eval_steps_per_second': 16.562, 'epoch': 8.0}\n",
            "{'loss': 0.4437, 'learning_rate': 2e-05, 'epoch': 8.41}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:07:56,102 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:07:56,103 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:07:56,104 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:07:56,104 >>   Batch size = 16\n",
            "01/07/2023 20:08:00 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5193819999694824, 'eval_matthews_correlation': 0.4302323404760539, 'eval_runtime': 3.9943, 'eval_samples_per_second': 261.125, 'eval_steps_per_second': 16.524, 'epoch': 9.0}\n",
            "{'loss': 0.4292, 'learning_rate': 2e-05, 'epoch': 9.35}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:08:35,301 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:08:35,303 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:08:35,303 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:08:35,303 >>   Batch size = 16\n",
            "01/07/2023 20:08:39 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5311723351478577, 'eval_matthews_correlation': 0.4126415451346723, 'eval_runtime': 4.0008, 'eval_samples_per_second': 260.696, 'eval_steps_per_second': 16.497, 'epoch': 10.0}\n",
            "{'loss': 0.4255, 'learning_rate': 2e-05, 'epoch': 10.28}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:09:15,006 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:09:15,008 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:09:15,008 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:09:15,008 >>   Batch size = 16\n",
            "01/07/2023 20:09:18 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5054172277450562, 'eval_matthews_correlation': 0.45532057993228, 'eval_runtime': 3.9845, 'eval_samples_per_second': 261.764, 'eval_steps_per_second': 16.564, 'epoch': 11.0}\n",
            "{'loss': 0.4338, 'learning_rate': 2e-05, 'epoch': 11.21}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:09:54,241 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:09:54,243 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:09:54,243 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:09:54,243 >>   Batch size = 16\n",
            "01/07/2023 20:09:58 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5358412861824036, 'eval_matthews_correlation': 0.41559945168273055, 'eval_runtime': 3.9854, 'eval_samples_per_second': 261.703, 'eval_steps_per_second': 16.56, 'epoch': 12.0}\n",
            "{'loss': 0.4153, 'learning_rate': 2e-05, 'epoch': 12.15}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:10:33,488 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:10:33,490 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:10:33,490 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:10:33,490 >>   Batch size = 16\n",
            "01/07/2023 20:10:37 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5034413933753967, 'eval_matthews_correlation': 0.4664784054861729, 'eval_runtime': 3.9829, 'eval_samples_per_second': 261.87, 'eval_steps_per_second': 16.571, 'epoch': 13.0}\n",
            "{'loss': 0.4214, 'learning_rate': 2e-05, 'epoch': 13.08}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:11:12,660 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:11:12,662 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:11:12,662 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:11:12,662 >>   Batch size = 16\n",
            "01/07/2023 20:11:16 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5412756204605103, 'eval_matthews_correlation': 0.42241607930644726, 'eval_runtime': 3.9962, 'eval_samples_per_second': 261.0, 'eval_steps_per_second': 16.516, 'epoch': 14.0}\n",
            "{'loss': 0.4168, 'learning_rate': 2e-05, 'epoch': 14.02}\n",
            "{'loss': 0.4059, 'learning_rate': 2e-05, 'epoch': 14.95}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:11:51,796 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:11:51,798 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:11:51,798 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:11:51,798 >>   Batch size = 16\n",
            "01/07/2023 20:11:55 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5150225162506104, 'eval_matthews_correlation': 0.4471424664690105, 'eval_runtime': 3.995, 'eval_samples_per_second': 261.078, 'eval_steps_per_second': 16.521, 'epoch': 15.0}\n",
            "[INFO|trainer.py:1530] 2023-01-07 20:11:55,793 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 580.7484, 'train_samples_per_second': 220.862, 'train_steps_per_second': 13.818, 'train_loss': 0.4551149579817632, 'epoch': 15.0}\n",
            "[INFO|configuration_utils.py:441] 2023-01-07 20:11:55,794 >> Configuration saved in ../../output/config.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       15.0\n",
            "  train_loss               =     0.4551\n",
            "  train_runtime            = 0:09:40.74\n",
            "  train_samples            =       8551\n",
            "  train_samples_per_second =    220.862\n",
            "  train_steps_per_second   =     13.818\n",
            "01/07/2023 20:11:55 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:566] 2023-01-07 20:11:55,795 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:11:55,797 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:11:55,797 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:11:55,797 >>   Batch size = 16\n",
            "01/07/2023 20:11:59 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5150225162506104, 'eval_matthews_correlation': 0.4471424664690105, 'eval_runtime': 3.9849, 'eval_samples_per_second': 261.741, 'eval_steps_per_second': 16.563, 'epoch': 15.0}\n",
            "***** eval metrics *****\n",
            "  epoch                     =       15.0\n",
            "  eval_loss                 =      0.515\n",
            "  eval_matthews_correlation =     0.4471\n",
            "  eval_runtime              = 0:00:03.98\n",
            "  eval_samples              =       1043\n",
            "  eval_samples_per_second   =    261.741\n",
            "  eval_steps_per_second     =     16.563\n",
            "01/07/2023 20:12:02 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "01/07/2023 20:12:02 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=True,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=../../output/runs/Jan07_20-12-02_6dbe8a223432,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.CONSTANT,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=15.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=../../output/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=16,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=[],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=../../output/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.NO,\n",
            "save_total_limit=0,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "01/07/2023 20:12:03 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "01/07/2023 20:12:03 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "01/07/2023 20:12:03 - INFO - datasets.info - Loading Dataset info from ../../cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "01/07/2023 20:12:03 - WARNING - datasets.builder - Found cached dataset glue (/content/papa/transformers/papa_scripts/../../cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "01/07/2023 20:12:03 - INFO - datasets.info - Loading Dataset info from /content/papa/transformers/papa_scripts/../../cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "100% 3/3 [00:00<00:00, 906.68it/s]\n",
            "[INFO|configuration_utils.py:654] 2023-01-07 20:12:03,609 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at ../../cache/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:690] 2023-01-07 20:12:03,609 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_seq_length\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sort_calculating\": null,\n",
            "  \"static_heads\": null,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"use_freeze_extract_pooler\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2023-01-07 20:12:03,834 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at ../../cache/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:690] 2023-01-07 20:12:03,835 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_seq_length\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sort_calculating\": null,\n",
            "  \"static_heads\": null,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"use_freeze_extract_pooler\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 20:12:04,505 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at ../../cache/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 20:12:04,505 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at ../../cache/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 20:12:04,505 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 20:12:04,505 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 20:12:04,505 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at ../../cache/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|configuration_utils.py:654] 2023-01-07 20:12:04,617 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at ../../cache/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:690] 2023-01-07 20:12:04,618 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_seq_length\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sort_calculating\": null,\n",
            "  \"static_heads\": null,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"use_freeze_extract_pooler\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1430] 2023-01-07 20:12:04,781 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at ../../cache/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[WARNING|modeling_utils.py:1688] 2023-01-07 20:12:06,388 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'bert.pooler.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.pooler.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1699] 2023-01-07 20:12:06,388 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.3.attention.self.static_heads_content', 'bert.encoder.layer.0.attention.self.static_heads_content', 'bert.encoder.layer.10.attention.self.static_heads_content', 'bert.pooler.weights_per_layer', 'bert.pooler.dense_across_layers.weight', 'bert.pooler.dense_across_layers.bias', 'bert.encoder.layer.11.attention.self.static_heads_content', 'bert.encoder.layer.9.attention.self.static_heads_content', 'bert.encoder.layer.2.attention.self.static_heads_content', 'bert.encoder.layer.4.attention.self.static_heads_content', 'bert.encoder.layer.6.attention.self.static_heads_content', 'bert.encoder.layer.5.attention.self.static_heads_content', 'bert.encoder.layer.1.attention.self.static_heads_content', 'bert.encoder.layer.8.attention.self.static_heads_content', 'bert.encoder.layer.7.attention.self.static_heads_content', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Not frozen param:  bert.pooler.weights_per_layer\n",
            "Not frozen param:  bert.pooler.dense_across_layers.weight\n",
            "Not frozen param:  bert.pooler.dense_across_layers.bias\n",
            "Not frozen param:  classifier.weight\n",
            "Not frozen param:  classifier.bias\n",
            "01/07/2023 20:12:06 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/papa/cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-89b22c6772fe5dc5.arrow\n",
            "01/07/2023 20:12:06 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/papa/cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-9bb9ed57acfe3164.arrow\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]01/07/2023 20:12:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/papa/cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-69b05663891aa8d0.arrow\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00, 13.98ba/s]\n",
            "01/07/2023 20:12:06 - INFO - __main__ - Sample 1824 of the training set: {'sentence': 'I acknowledged that my father, he was tight as an owl.', 'label': 0, 'idx': 1824, 'input_ids': [101, 1045, 8969, 2008, 2026, 2269, 1010, 2002, 2001, 4389, 2004, 2019, 13547, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'old_input_ids': [101, 1045, 8969, 2008, 2026, 2269, 1010, 2002, 2001, 4389, 2004, 2019, 13547, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'old_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "01/07/2023 20:12:06 - INFO - __main__ - Sample 409 of the training set: {'sentence': 'For him to do that would be a mistake.', 'label': 1, 'idx': 409, 'input_ids': [101, 2005, 2032, 2000, 2079, 2008, 2052, 2022, 1037, 6707, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'old_input_ids': [101, 2005, 2032, 2000, 2079, 2008, 2052, 2022, 1037, 6707, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'old_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "01/07/2023 20:12:06 - INFO - __main__ - Sample 4506 of the training set: {'sentence': 'Mary sang a song, but Lee never did.', 'label': 1, 'idx': 4506, 'input_ids': [101, 2984, 6369, 1037, 2299, 1010, 2021, 3389, 2196, 2106, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'old_input_ids': [101, 2984, 6369, 1037, 2299, 1010, 2021, 3389, 2196, 2106, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'old_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "run_papa_glue.py:567: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"glue\", data_args.task_name)\n",
            "[INFO|trainer.py:566] 2023-01-07 20:12:09,654 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, old_attention_mask, sentence, old_input_ids. If idx, old_attention_mask, sentence, old_input_ids are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/envs/env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1290] 2023-01-07 20:12:09,664 >> ***** Running training *****\n",
            "[INFO|trainer.py:1291] 2023-01-07 20:12:09,664 >>   Num examples = 8551\n",
            "[INFO|trainer.py:1292] 2023-01-07 20:12:09,664 >>   Num Epochs = 15\n",
            "[INFO|trainer.py:1293] 2023-01-07 20:12:09,664 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1294] 2023-01-07 20:12:09,664 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1295] 2023-01-07 20:12:09,664 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1296] 2023-01-07 20:12:09,664 >>   Total optimization steps = 8025\n",
            "{'loss': 0.5661, 'learning_rate': 2e-05, 'epoch': 0.93}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:12:45,132 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, old_attention_mask, sentence, old_input_ids. If idx, old_attention_mask, sentence, old_input_ids are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:12:45,134 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:12:45,134 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:12:45,134 >>   Batch size = 16\n",
            "01/07/2023 20:12:48 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5496065616607666, 'eval_matthews_correlation': 0.24479119182687428, 'eval_runtime': 3.8591, 'eval_samples_per_second': 270.267, 'eval_steps_per_second': 17.102, 'epoch': 1.0}\n",
            "{'loss': 0.5076, 'learning_rate': 2e-05, 'epoch': 1.87}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:13:22,236 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, old_attention_mask, sentence, old_input_ids. If idx, old_attention_mask, sentence, old_input_ids are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:13:22,238 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:13:22,238 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:13:22,238 >>   Batch size = 16\n",
            "01/07/2023 20:13:25 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5116384029388428, 'eval_matthews_correlation': 0.3445904443312878, 'eval_runtime': 3.7557, 'eval_samples_per_second': 277.714, 'eval_steps_per_second': 17.573, 'epoch': 2.0}\n",
            "{'loss': 0.4945, 'learning_rate': 2e-05, 'epoch': 2.8}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:13:59,090 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, old_attention_mask, sentence, old_input_ids. If idx, old_attention_mask, sentence, old_input_ids are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:13:59,092 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:13:59,092 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:13:59,092 >>   Batch size = 16\n",
            "01/07/2023 20:14:02 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5161762237548828, 'eval_matthews_correlation': 0.3587014171000859, 'eval_runtime': 3.801, 'eval_samples_per_second': 274.398, 'eval_steps_per_second': 17.364, 'epoch': 3.0}\n",
            "{'loss': 0.4747, 'learning_rate': 2e-05, 'epoch': 3.74}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:14:36,171 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, old_attention_mask, sentence, old_input_ids. If idx, old_attention_mask, sentence, old_input_ids are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:14:36,173 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:14:36,173 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:14:36,173 >>   Batch size = 16\n",
            "01/07/2023 20:14:39 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.49849483370780945, 'eval_matthews_correlation': 0.3976852361564824, 'eval_runtime': 3.782, 'eval_samples_per_second': 275.78, 'eval_steps_per_second': 17.451, 'epoch': 4.0}\n",
            "{'loss': 0.4713, 'learning_rate': 2e-05, 'epoch': 4.67}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:15:13,113 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, old_attention_mask, sentence, old_input_ids. If idx, old_attention_mask, sentence, old_input_ids are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:15:13,115 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:15:13,115 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:15:13,115 >>   Batch size = 16\n",
            "01/07/2023 20:15:16 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.4875124394893646, 'eval_matthews_correlation': 0.4122875932340276, 'eval_runtime': 3.7961, 'eval_samples_per_second': 274.757, 'eval_steps_per_second': 17.386, 'epoch': 5.0}\n",
            "{'loss': 0.4616, 'learning_rate': 2e-05, 'epoch': 5.61}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:15:51,384 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, old_attention_mask, sentence, old_input_ids. If idx, old_attention_mask, sentence, old_input_ids are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:15:51,386 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:15:51,386 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:15:51,386 >>   Batch size = 16\n",
            "01/07/2023 20:15:55 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5347082018852234, 'eval_matthews_correlation': 0.3968604425696697, 'eval_runtime': 3.7767, 'eval_samples_per_second': 276.169, 'eval_steps_per_second': 17.476, 'epoch': 6.0}\n",
            "{'loss': 0.4552, 'learning_rate': 2e-05, 'epoch': 6.54}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:16:28,362 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, old_attention_mask, sentence, old_input_ids. If idx, old_attention_mask, sentence, old_input_ids are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:16:28,364 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:16:28,364 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:16:28,364 >>   Batch size = 16\n",
            "01/07/2023 20:16:32 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5193562507629395, 'eval_matthews_correlation': 0.4070638887222338, 'eval_runtime': 3.7878, 'eval_samples_per_second': 275.355, 'eval_steps_per_second': 17.424, 'epoch': 7.0}\n",
            "{'loss': 0.4382, 'learning_rate': 2e-05, 'epoch': 7.48}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:17:05,365 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, old_attention_mask, sentence, old_input_ids. If idx, old_attention_mask, sentence, old_input_ids are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:17:05,367 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:17:05,367 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:17:05,367 >>   Batch size = 16\n",
            "01/07/2023 20:17:09 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5173091292381287, 'eval_matthews_correlation': 0.42213444679939793, 'eval_runtime': 3.7855, 'eval_samples_per_second': 275.522, 'eval_steps_per_second': 17.435, 'epoch': 8.0}\n",
            "{'loss': 0.4417, 'learning_rate': 2e-05, 'epoch': 8.41}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:17:42,305 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, old_attention_mask, sentence, old_input_ids. If idx, old_attention_mask, sentence, old_input_ids are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:17:42,307 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:17:42,307 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:17:42,307 >>   Batch size = 16\n",
            "01/07/2023 20:17:46 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.49874991178512573, 'eval_matthews_correlation': 0.44429652763655303, 'eval_runtime': 3.7885, 'eval_samples_per_second': 275.304, 'eval_steps_per_second': 17.421, 'epoch': 9.0}\n",
            "{'loss': 0.4263, 'learning_rate': 2e-05, 'epoch': 9.35}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:18:19,237 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, old_attention_mask, sentence, old_input_ids. If idx, old_attention_mask, sentence, old_input_ids are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:18:19,239 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:18:19,239 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:18:19,239 >>   Batch size = 16\n",
            "01/07/2023 20:18:23 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5118038654327393, 'eval_matthews_correlation': 0.415328406505499, 'eval_runtime': 3.7829, 'eval_samples_per_second': 275.714, 'eval_steps_per_second': 17.447, 'epoch': 10.0}\n",
            "{'loss': 0.4239, 'learning_rate': 2e-05, 'epoch': 10.28}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:18:57,416 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, old_attention_mask, sentence, old_input_ids. If idx, old_attention_mask, sentence, old_input_ids are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:18:57,418 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:18:57,418 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:18:57,418 >>   Batch size = 16\n",
            "01/07/2023 20:19:01 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.47857901453971863, 'eval_matthews_correlation': 0.45808021085661066, 'eval_runtime': 3.8264, 'eval_samples_per_second': 272.577, 'eval_steps_per_second': 17.248, 'epoch': 11.0}\n",
            "{'loss': 0.4302, 'learning_rate': 2e-05, 'epoch': 11.21}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:19:34,432 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, old_attention_mask, sentence, old_input_ids. If idx, old_attention_mask, sentence, old_input_ids are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:19:34,434 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:19:34,434 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:19:34,434 >>   Batch size = 16\n",
            "01/07/2023 20:19:38 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.509185791015625, 'eval_matthews_correlation': 0.4501311991170613, 'eval_runtime': 3.7904, 'eval_samples_per_second': 275.171, 'eval_steps_per_second': 17.413, 'epoch': 12.0}\n",
            "{'loss': 0.4119, 'learning_rate': 2e-05, 'epoch': 12.15}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:20:11,479 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, old_attention_mask, sentence, old_input_ids. If idx, old_attention_mask, sentence, old_input_ids are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:20:11,481 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:20:11,481 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:20:11,481 >>   Batch size = 16\n",
            "01/07/2023 20:20:15 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.48107418417930603, 'eval_matthews_correlation': 0.466428275513655, 'eval_runtime': 3.7919, 'eval_samples_per_second': 275.061, 'eval_steps_per_second': 17.406, 'epoch': 13.0}\n",
            "{'loss': 0.419, 'learning_rate': 2e-05, 'epoch': 13.08}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:20:48,541 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, old_attention_mask, sentence, old_input_ids. If idx, old_attention_mask, sentence, old_input_ids are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:20:48,543 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:20:48,543 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:20:48,543 >>   Batch size = 16\n",
            "01/07/2023 20:20:52 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5069959163665771, 'eval_matthews_correlation': 0.44158736727777204, 'eval_runtime': 3.7947, 'eval_samples_per_second': 274.858, 'eval_steps_per_second': 17.393, 'epoch': 14.0}\n",
            "{'loss': 0.4131, 'learning_rate': 2e-05, 'epoch': 14.02}\n",
            "{'loss': 0.4025, 'learning_rate': 2e-05, 'epoch': 14.95}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:21:25,611 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, old_attention_mask, sentence, old_input_ids. If idx, old_attention_mask, sentence, old_input_ids are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:21:25,613 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:21:25,613 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:21:25,613 >>   Batch size = 16\n",
            "01/07/2023 20:21:29 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.4836111068725586, 'eval_matthews_correlation': 0.4636216536931484, 'eval_runtime': 3.7845, 'eval_samples_per_second': 275.6, 'eval_steps_per_second': 17.44, 'epoch': 15.0}\n",
            "[INFO|trainer.py:1530] 2023-01-07 20:21:29,398 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 559.7335, 'train_samples_per_second': 229.154, 'train_steps_per_second': 14.337, 'train_loss': 0.45215945781577044, 'epoch': 15.0}\n",
            "[INFO|configuration_utils.py:441] 2023-01-07 20:21:29,399 >> Configuration saved in ../../output/config.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       15.0\n",
            "  train_loss               =     0.4522\n",
            "  train_runtime            = 0:09:19.73\n",
            "  train_samples            =       8551\n",
            "  train_samples_per_second =    229.154\n",
            "  train_steps_per_second   =     14.337\n",
            "01/07/2023 20:21:29 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:566] 2023-01-07 20:21:29,401 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, old_attention_mask, sentence, old_input_ids. If idx, old_attention_mask, sentence, old_input_ids are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:21:29,402 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:21:29,403 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:21:29,403 >>   Batch size = 16\n",
            "01/07/2023 20:21:33 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.4836111068725586, 'eval_matthews_correlation': 0.4636216536931484, 'eval_runtime': 3.78, 'eval_samples_per_second': 275.924, 'eval_steps_per_second': 17.46, 'epoch': 15.0}\n",
            "***** eval metrics *****\n",
            "  epoch                     =       15.0\n",
            "  eval_loss                 =     0.4836\n",
            "  eval_matthews_correlation =     0.4636\n",
            "  eval_runtime              = 0:00:03.77\n",
            "  eval_samples              =       1043\n",
            "  eval_samples_per_second   =    275.924\n",
            "  eval_steps_per_second     =      17.46\n",
            "01/07/2023 20:21:36 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "01/07/2023 20:21:36 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=True,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=../../output/runs/Jan07_20-21-35_6dbe8a223432,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.CONSTANT,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=15.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=../../output/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=16,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=[],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=../../output/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.NO,\n",
            "save_total_limit=0,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "01/07/2023 20:21:36 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "01/07/2023 20:21:36 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "01/07/2023 20:21:36 - INFO - datasets.info - Loading Dataset info from ../../cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "01/07/2023 20:21:36 - WARNING - datasets.builder - Found cached dataset glue (/content/papa/transformers/papa_scripts/../../cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "01/07/2023 20:21:36 - INFO - datasets.info - Loading Dataset info from /content/papa/transformers/papa_scripts/../../cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "100% 3/3 [00:00<00:00, 972.40it/s]\n",
            "[INFO|configuration_utils.py:654] 2023-01-07 20:21:36,998 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at ../../cache/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:690] 2023-01-07 20:21:36,999 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_seq_length\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sort_calculating\": null,\n",
            "  \"static_heads\": null,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"use_freeze_extract_pooler\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2023-01-07 20:21:37,226 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at ../../cache/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:690] 2023-01-07 20:21:37,227 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_seq_length\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sort_calculating\": null,\n",
            "  \"static_heads\": null,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"use_freeze_extract_pooler\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 20:21:37,917 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at ../../cache/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 20:21:37,917 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at ../../cache/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 20:21:37,917 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 20:21:37,917 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 20:21:37,917 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at ../../cache/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|configuration_utils.py:654] 2023-01-07 20:21:38,029 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at ../../cache/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:690] 2023-01-07 20:21:38,030 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_seq_length\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sort_calculating\": null,\n",
            "  \"static_heads\": null,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"use_freeze_extract_pooler\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1430] 2023-01-07 20:21:38,190 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at ../../cache/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[WARNING|modeling_utils.py:1688] 2023-01-07 20:21:39,825 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'bert.pooler.dense.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1699] 2023-01-07 20:21:39,825 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.2.attention.self.static_heads_content', 'bert.encoder.layer.9.attention.self.static_heads_content', 'bert.pooler.dense_across_layers.weight', 'bert.encoder.layer.10.attention.self.static_heads_content', 'bert.encoder.layer.1.attention.self.static_heads_content', 'bert.encoder.layer.3.attention.self.static_heads_content', 'classifier.bias', 'bert.encoder.layer.6.attention.self.static_heads_content', 'bert.encoder.layer.8.attention.self.static_heads_content', 'classifier.weight', 'bert.encoder.layer.7.attention.self.static_heads_content', 'bert.pooler.dense_across_layers.bias', 'bert.encoder.layer.4.attention.self.static_heads_content', 'bert.encoder.layer.11.attention.self.static_heads_content', 'bert.encoder.layer.0.attention.self.static_heads_content', 'bert.pooler.weights_per_layer', 'bert.encoder.layer.5.attention.self.static_heads_content']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Not frozen param:  bert.pooler.weights_per_layer\n",
            "Not frozen param:  bert.pooler.dense_across_layers.weight\n",
            "Not frozen param:  bert.pooler.dense_across_layers.bias\n",
            "Not frozen param:  classifier.weight\n",
            "Not frozen param:  classifier.bias\n",
            "01/07/2023 20:21:39 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/papa/cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-89b22c6772fe5dc5.arrow\n",
            "01/07/2023 20:21:39 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/papa/cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-9bb9ed57acfe3164.arrow\n",
            "01/07/2023 20:21:39 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/papa/cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-69b05663891aa8d0.arrow\n",
            "01/07/2023 20:21:39 - INFO - __main__ - Sample 1824 of the training set: {'sentence': 'I acknowledged that my father, he was tight as an owl.', 'label': 0, 'idx': 1824, 'input_ids': [101, 1045, 8969, 2008, 2026, 2269, 1010, 2002, 2001, 4389, 2004, 2019, 13547, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'old_input_ids': [101, 1045, 8969, 2008, 2026, 2269, 1010, 2002, 2001, 4389, 2004, 2019, 13547, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'old_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "01/07/2023 20:21:39 - INFO - __main__ - Sample 409 of the training set: {'sentence': 'For him to do that would be a mistake.', 'label': 1, 'idx': 409, 'input_ids': [101, 2005, 2032, 2000, 2079, 2008, 2052, 2022, 1037, 6707, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'old_input_ids': [101, 2005, 2032, 2000, 2079, 2008, 2052, 2022, 1037, 6707, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'old_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "01/07/2023 20:21:39 - INFO - __main__ - Sample 4506 of the training set: {'sentence': 'Mary sang a song, but Lee never did.', 'label': 1, 'idx': 4506, 'input_ids': [101, 2984, 6369, 1037, 2299, 1010, 2021, 3389, 2196, 2106, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'old_input_ids': [101, 2984, 6369, 1037, 2299, 1010, 2021, 3389, 2196, 2106, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'old_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "run_papa_glue.py:567: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"glue\", data_args.task_name)\n",
            "[INFO|trainer.py:566] 2023-01-07 20:21:42,749 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/envs/env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1290] 2023-01-07 20:21:42,757 >> ***** Running training *****\n",
            "[INFO|trainer.py:1291] 2023-01-07 20:21:42,757 >>   Num examples = 8551\n",
            "[INFO|trainer.py:1292] 2023-01-07 20:21:42,757 >>   Num Epochs = 15\n",
            "[INFO|trainer.py:1293] 2023-01-07 20:21:42,757 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1294] 2023-01-07 20:21:42,757 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1295] 2023-01-07 20:21:42,757 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1296] 2023-01-07 20:21:42,757 >>   Total optimization steps = 8025\n",
            "{'loss': 0.5846, 'learning_rate': 2e-05, 'epoch': 0.93}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:22:15,538 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:22:15,540 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:22:15,540 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:22:15,540 >>   Batch size = 16\n",
            "01/07/2023 20:22:19 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5760383009910583, 'eval_matthews_correlation': 0.08436523968200914, 'eval_runtime': 3.8063, 'eval_samples_per_second': 274.016, 'eval_steps_per_second': 17.339, 'epoch': 1.0}\n",
            "{'loss': 0.5439, 'learning_rate': 2e-05, 'epoch': 1.87}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:22:50,721 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:22:50,722 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:22:50,722 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:22:50,722 >>   Batch size = 16\n",
            "01/07/2023 20:22:54 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5514234900474548, 'eval_matthews_correlation': 0.26958051829755825, 'eval_runtime': 3.4905, 'eval_samples_per_second': 298.812, 'eval_steps_per_second': 18.909, 'epoch': 2.0}\n",
            "{'loss': 0.5353, 'learning_rate': 2e-05, 'epoch': 2.8}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:23:25,427 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:23:25,429 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:23:25,430 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:23:25,430 >>   Batch size = 16\n",
            "01/07/2023 20:23:28 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5500823259353638, 'eval_matthews_correlation': 0.29129508625279804, 'eval_runtime': 3.5073, 'eval_samples_per_second': 297.379, 'eval_steps_per_second': 18.818, 'epoch': 3.0}\n",
            "{'loss': 0.5189, 'learning_rate': 2e-05, 'epoch': 3.74}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:24:00,375 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:24:00,376 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:24:00,377 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:24:00,377 >>   Batch size = 16\n",
            "01/07/2023 20:24:03 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5337812900543213, 'eval_matthews_correlation': 0.3448772332310962, 'eval_runtime': 3.5084, 'eval_samples_per_second': 297.287, 'eval_steps_per_second': 18.812, 'epoch': 4.0}\n",
            "{'loss': 0.5113, 'learning_rate': 2e-05, 'epoch': 4.67}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:24:35,281 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:24:35,283 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:24:35,283 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:24:35,283 >>   Batch size = 16\n",
            "01/07/2023 20:24:38 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5261194705963135, 'eval_matthews_correlation': 0.347756413383896, 'eval_runtime': 3.4945, 'eval_samples_per_second': 298.467, 'eval_steps_per_second': 18.887, 'epoch': 5.0}\n",
            "{'loss': 0.5027, 'learning_rate': 2e-05, 'epoch': 5.61}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:25:10,163 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:25:10,164 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:25:10,165 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:25:10,165 >>   Batch size = 16\n",
            "01/07/2023 20:25:13 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5465690493583679, 'eval_matthews_correlation': 0.33284665338171543, 'eval_runtime': 3.511, 'eval_samples_per_second': 297.065, 'eval_steps_per_second': 18.798, 'epoch': 6.0}\n",
            "{'loss': 0.4949, 'learning_rate': 2e-05, 'epoch': 6.54}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:25:46,264 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:25:46,266 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:25:46,266 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:25:46,266 >>   Batch size = 16\n",
            "01/07/2023 20:25:49 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5495893955230713, 'eval_matthews_correlation': 0.33250229746942567, 'eval_runtime': 3.5095, 'eval_samples_per_second': 297.192, 'eval_steps_per_second': 18.806, 'epoch': 7.0}\n",
            "{'loss': 0.4796, 'learning_rate': 2e-05, 'epoch': 7.48}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:26:21,120 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:26:21,121 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:26:21,122 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:26:21,122 >>   Batch size = 16\n",
            "01/07/2023 20:26:24 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5394027233123779, 'eval_matthews_correlation': 0.35545278217642234, 'eval_runtime': 3.4919, 'eval_samples_per_second': 298.695, 'eval_steps_per_second': 18.901, 'epoch': 8.0}\n",
            "{'loss': 0.4846, 'learning_rate': 2e-05, 'epoch': 8.41}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:26:55,977 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:26:55,980 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:26:55,980 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:26:55,980 >>   Batch size = 16\n",
            "01/07/2023 20:26:59 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5239460468292236, 'eval_matthews_correlation': 0.39745964126329725, 'eval_runtime': 3.5191, 'eval_samples_per_second': 296.384, 'eval_steps_per_second': 18.755, 'epoch': 9.0}\n",
            "{'loss': 0.4728, 'learning_rate': 2e-05, 'epoch': 9.35}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:27:31,057 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:27:31,058 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:27:31,058 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:27:31,058 >>   Batch size = 16\n",
            "01/07/2023 20:27:34 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5348408222198486, 'eval_matthews_correlation': 0.36062118925968195, 'eval_runtime': 3.5133, 'eval_samples_per_second': 296.871, 'eval_steps_per_second': 18.786, 'epoch': 10.0}\n",
            "{'loss': 0.4626, 'learning_rate': 2e-05, 'epoch': 10.28}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:28:06,077 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:28:06,078 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:28:06,078 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:28:06,079 >>   Batch size = 16\n",
            "01/07/2023 20:28:09 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5197609066963196, 'eval_matthews_correlation': 0.4063400389101911, 'eval_runtime': 3.5088, 'eval_samples_per_second': 297.251, 'eval_steps_per_second': 18.81, 'epoch': 11.0}\n",
            "{'loss': 0.4722, 'learning_rate': 2e-05, 'epoch': 11.21}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:28:41,052 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:28:41,054 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:28:41,054 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:28:41,054 >>   Batch size = 16\n",
            "01/07/2023 20:28:44 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5301920771598816, 'eval_matthews_correlation': 0.4123772629669354, 'eval_runtime': 3.5182, 'eval_samples_per_second': 296.459, 'eval_steps_per_second': 18.76, 'epoch': 12.0}\n",
            "{'loss': 0.4542, 'learning_rate': 2e-05, 'epoch': 12.15}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:29:17,308 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:29:17,310 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:29:17,310 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:29:17,310 >>   Batch size = 16\n",
            "01/07/2023 20:29:20 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5176159739494324, 'eval_matthews_correlation': 0.40119410686315105, 'eval_runtime': 3.5002, 'eval_samples_per_second': 297.984, 'eval_steps_per_second': 18.856, 'epoch': 13.0}\n",
            "{'loss': 0.4609, 'learning_rate': 2e-05, 'epoch': 13.08}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:29:52,087 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:29:52,089 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:29:52,089 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:29:52,089 >>   Batch size = 16\n",
            "01/07/2023 20:29:55 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5347952842712402, 'eval_matthews_correlation': 0.4128182191738021, 'eval_runtime': 3.5052, 'eval_samples_per_second': 297.561, 'eval_steps_per_second': 18.829, 'epoch': 14.0}\n",
            "{'loss': 0.4528, 'learning_rate': 2e-05, 'epoch': 14.02}\n",
            "{'loss': 0.4473, 'learning_rate': 2e-05, 'epoch': 14.95}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:30:26,891 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:30:26,893 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:30:26,893 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:30:26,893 >>   Batch size = 16\n",
            "01/07/2023 20:30:30 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5180580019950867, 'eval_matthews_correlation': 0.3986722481537282, 'eval_runtime': 3.4999, 'eval_samples_per_second': 298.006, 'eval_steps_per_second': 18.857, 'epoch': 15.0}\n",
            "[INFO|trainer.py:1530] 2023-01-07 20:30:30,393 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 527.6356, 'train_samples_per_second': 243.094, 'train_steps_per_second': 15.209, 'train_loss': 0.4922437664281542, 'epoch': 15.0}\n",
            "[INFO|configuration_utils.py:441] 2023-01-07 20:30:30,394 >> Configuration saved in ../../output/config.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       15.0\n",
            "  train_loss               =     0.4922\n",
            "  train_runtime            = 0:08:47.63\n",
            "  train_samples            =       8551\n",
            "  train_samples_per_second =    243.094\n",
            "  train_steps_per_second   =     15.209\n",
            "01/07/2023 20:30:30 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:566] 2023-01-07 20:30:30,396 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_input_ids, sentence, old_attention_mask, idx. If old_input_ids, sentence, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:30:30,397 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:30:30,397 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:30:30,397 >>   Batch size = 16\n",
            "01/07/2023 20:30:33 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5180580019950867, 'eval_matthews_correlation': 0.3986722481537282, 'eval_runtime': 3.5052, 'eval_samples_per_second': 297.559, 'eval_steps_per_second': 18.829, 'epoch': 15.0}\n",
            "***** eval metrics *****\n",
            "  epoch                     =       15.0\n",
            "  eval_loss                 =     0.5181\n",
            "  eval_matthews_correlation =     0.3987\n",
            "  eval_runtime              = 0:00:03.50\n",
            "  eval_samples              =       1043\n",
            "  eval_samples_per_second   =    297.559\n",
            "  eval_steps_per_second     =     18.829\n",
            "01/07/2023 20:30:36 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "01/07/2023 20:30:36 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=True,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=../../output/runs/Jan07_20-30-36_6dbe8a223432,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.CONSTANT,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=15.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=../../output/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=16,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=[],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=../../output/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.NO,\n",
            "save_total_limit=0,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "01/07/2023 20:30:37 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "01/07/2023 20:30:37 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "01/07/2023 20:30:37 - INFO - datasets.info - Loading Dataset info from ../../cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "01/07/2023 20:30:37 - WARNING - datasets.builder - Found cached dataset glue (/content/papa/transformers/papa_scripts/../../cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "01/07/2023 20:30:37 - INFO - datasets.info - Loading Dataset info from /content/papa/transformers/papa_scripts/../../cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "100% 3/3 [00:00<00:00, 973.83it/s]\n",
            "[INFO|configuration_utils.py:654] 2023-01-07 20:30:37,662 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at ../../cache/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:690] 2023-01-07 20:30:37,663 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_seq_length\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sort_calculating\": null,\n",
            "  \"static_heads\": null,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"use_freeze_extract_pooler\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2023-01-07 20:30:37,886 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at ../../cache/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:690] 2023-01-07 20:30:37,887 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_seq_length\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sort_calculating\": null,\n",
            "  \"static_heads\": null,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"use_freeze_extract_pooler\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 20:30:38,558 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at ../../cache/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 20:30:38,559 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at ../../cache/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 20:30:38,559 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 20:30:38,559 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 20:30:38,559 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at ../../cache/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|configuration_utils.py:654] 2023-01-07 20:30:38,670 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at ../../cache/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:690] 2023-01-07 20:30:38,671 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_seq_length\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sort_calculating\": null,\n",
            "  \"static_heads\": null,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"use_freeze_extract_pooler\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1430] 2023-01-07 20:30:38,830 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at ../../cache/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[WARNING|modeling_utils.py:1688] 2023-01-07 20:30:40,421 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1699] 2023-01-07 20:30:40,421 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.pooler.dense_across_layers.bias', 'bert.encoder.layer.3.attention.self.static_heads_content', 'bert.encoder.layer.0.attention.self.static_heads_content', 'bert.encoder.layer.7.attention.self.static_heads_content', 'bert.encoder.layer.11.attention.self.static_heads_content', 'bert.pooler.weights_per_layer', 'bert.encoder.layer.10.attention.self.static_heads_content', 'classifier.bias', 'bert.pooler.dense_across_layers.weight', 'bert.encoder.layer.6.attention.self.static_heads_content', 'bert.encoder.layer.2.attention.self.static_heads_content', 'bert.encoder.layer.4.attention.self.static_heads_content', 'bert.encoder.layer.1.attention.self.static_heads_content', 'classifier.weight', 'bert.encoder.layer.9.attention.self.static_heads_content', 'bert.encoder.layer.5.attention.self.static_heads_content', 'bert.encoder.layer.8.attention.self.static_heads_content']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Not frozen param:  bert.pooler.weights_per_layer\n",
            "Not frozen param:  bert.pooler.dense_across_layers.weight\n",
            "Not frozen param:  bert.pooler.dense_across_layers.bias\n",
            "Not frozen param:  classifier.weight\n",
            "Not frozen param:  classifier.bias\n",
            "01/07/2023 20:30:40 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/papa/cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-89b22c6772fe5dc5.arrow\n",
            "01/07/2023 20:30:40 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/papa/cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-9bb9ed57acfe3164.arrow\n",
            "01/07/2023 20:30:40 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/papa/cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-69b05663891aa8d0.arrow\n",
            "01/07/2023 20:30:40 - INFO - __main__ - Sample 1824 of the training set: {'sentence': 'I acknowledged that my father, he was tight as an owl.', 'label': 0, 'idx': 1824, 'input_ids': [101, 1045, 8969, 2008, 2026, 2269, 1010, 2002, 2001, 4389, 2004, 2019, 13547, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'old_input_ids': [101, 1045, 8969, 2008, 2026, 2269, 1010, 2002, 2001, 4389, 2004, 2019, 13547, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'old_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "01/07/2023 20:30:40 - INFO - __main__ - Sample 409 of the training set: {'sentence': 'For him to do that would be a mistake.', 'label': 1, 'idx': 409, 'input_ids': [101, 2005, 2032, 2000, 2079, 2008, 2052, 2022, 1037, 6707, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'old_input_ids': [101, 2005, 2032, 2000, 2079, 2008, 2052, 2022, 1037, 6707, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'old_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "01/07/2023 20:30:40 - INFO - __main__ - Sample 4506 of the training set: {'sentence': 'Mary sang a song, but Lee never did.', 'label': 1, 'idx': 4506, 'input_ids': [101, 2984, 6369, 1037, 2299, 1010, 2021, 3389, 2196, 2106, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'old_input_ids': [101, 2984, 6369, 1037, 2299, 1010, 2021, 3389, 2196, 2106, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'old_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "run_papa_glue.py:567: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"glue\", data_args.task_name)\n",
            "[INFO|trainer.py:566] 2023-01-07 20:30:43,301 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_attention_mask, idx, old_input_ids, sentence. If old_attention_mask, idx, old_input_ids, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/envs/env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1290] 2023-01-07 20:30:43,309 >> ***** Running training *****\n",
            "[INFO|trainer.py:1291] 2023-01-07 20:30:43,309 >>   Num examples = 8551\n",
            "[INFO|trainer.py:1292] 2023-01-07 20:30:43,309 >>   Num Epochs = 15\n",
            "[INFO|trainer.py:1293] 2023-01-07 20:30:43,309 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1294] 2023-01-07 20:30:43,309 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1295] 2023-01-07 20:30:43,309 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1296] 2023-01-07 20:30:43,309 >>   Total optimization steps = 8025\n",
            "{'loss': 0.5877, 'learning_rate': 2e-05, 'epoch': 0.93}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:31:14,726 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_attention_mask, idx, old_input_ids, sentence. If old_attention_mask, idx, old_input_ids, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:31:14,728 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:31:14,728 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:31:14,728 >>   Batch size = 16\n",
            "01/07/2023 20:31:18 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5778442621231079, 'eval_matthews_correlation': 0.0835915715371112, 'eval_runtime': 3.5325, 'eval_samples_per_second': 295.261, 'eval_steps_per_second': 18.684, 'epoch': 1.0}\n",
            "{'loss': 0.5488, 'learning_rate': 2e-05, 'epoch': 1.87}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:31:49,203 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_attention_mask, idx, old_input_ids, sentence. If old_attention_mask, idx, old_input_ids, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:31:49,205 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:31:49,205 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:31:49,205 >>   Batch size = 16\n",
            "01/07/2023 20:31:52 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5545588135719299, 'eval_matthews_correlation': 0.24107986637863602, 'eval_runtime': 3.4448, 'eval_samples_per_second': 302.772, 'eval_steps_per_second': 19.159, 'epoch': 2.0}\n",
            "{'loss': 0.5401, 'learning_rate': 2e-05, 'epoch': 2.8}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:32:24,374 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_attention_mask, idx, old_input_ids, sentence. If old_attention_mask, idx, old_input_ids, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:32:24,376 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:32:24,376 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:32:24,376 >>   Batch size = 16\n",
            "01/07/2023 20:32:27 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5508899688720703, 'eval_matthews_correlation': 0.25610316519714893, 'eval_runtime': 3.4458, 'eval_samples_per_second': 302.692, 'eval_steps_per_second': 19.154, 'epoch': 3.0}\n",
            "{'loss': 0.5252, 'learning_rate': 2e-05, 'epoch': 3.74}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:32:58,632 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_attention_mask, idx, old_input_ids, sentence. If old_attention_mask, idx, old_input_ids, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:32:58,634 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:32:58,634 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:32:58,634 >>   Batch size = 16\n",
            "01/07/2023 20:33:02 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5367158651351929, 'eval_matthews_correlation': 0.3145063521545815, 'eval_runtime': 3.4541, 'eval_samples_per_second': 301.961, 'eval_steps_per_second': 19.108, 'epoch': 4.0}\n",
            "{'loss': 0.5151, 'learning_rate': 2e-05, 'epoch': 4.67}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:33:32,767 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_attention_mask, idx, old_input_ids, sentence. If old_attention_mask, idx, old_input_ids, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:33:32,769 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:33:32,769 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:33:32,769 >>   Batch size = 16\n",
            "01/07/2023 20:33:36 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5310782194137573, 'eval_matthews_correlation': 0.3147232005559896, 'eval_runtime': 3.4492, 'eval_samples_per_second': 302.385, 'eval_steps_per_second': 19.135, 'epoch': 5.0}\n",
            "{'loss': 0.508, 'learning_rate': 2e-05, 'epoch': 5.61}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:34:06,974 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_attention_mask, idx, old_input_ids, sentence. If old_attention_mask, idx, old_input_ids, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:34:06,976 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:34:06,976 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:34:06,976 >>   Batch size = 16\n",
            "01/07/2023 20:34:10 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5478191375732422, 'eval_matthews_correlation': 0.3184881845473565, 'eval_runtime': 3.4524, 'eval_samples_per_second': 302.11, 'eval_steps_per_second': 19.117, 'epoch': 6.0}\n",
            "{'loss': 0.4996, 'learning_rate': 2e-05, 'epoch': 6.54}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:34:41,174 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_attention_mask, idx, old_input_ids, sentence. If old_attention_mask, idx, old_input_ids, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:34:41,176 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:34:41,176 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:34:41,176 >>   Batch size = 16\n",
            "01/07/2023 20:34:44 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5526772737503052, 'eval_matthews_correlation': 0.3114990754917698, 'eval_runtime': 3.4547, 'eval_samples_per_second': 301.908, 'eval_steps_per_second': 19.104, 'epoch': 7.0}\n",
            "{'loss': 0.4843, 'learning_rate': 2e-05, 'epoch': 7.48}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:35:15,401 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_attention_mask, idx, old_input_ids, sentence. If old_attention_mask, idx, old_input_ids, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:35:15,403 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:35:15,403 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:35:15,403 >>   Batch size = 16\n",
            "01/07/2023 20:35:18 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5410488843917847, 'eval_matthews_correlation': 0.3512420800619993, 'eval_runtime': 3.4608, 'eval_samples_per_second': 301.371, 'eval_steps_per_second': 19.07, 'epoch': 8.0}\n",
            "{'loss': 0.4897, 'learning_rate': 2e-05, 'epoch': 8.41}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:35:50,905 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_attention_mask, idx, old_input_ids, sentence. If old_attention_mask, idx, old_input_ids, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:35:50,907 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:35:50,907 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:35:50,907 >>   Batch size = 16\n",
            "01/07/2023 20:35:54 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.524792492389679, 'eval_matthews_correlation': 0.37653897052469565, 'eval_runtime': 3.4532, 'eval_samples_per_second': 302.043, 'eval_steps_per_second': 19.113, 'epoch': 9.0}\n",
            "{'loss': 0.4774, 'learning_rate': 2e-05, 'epoch': 9.35}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:36:25,070 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_attention_mask, idx, old_input_ids, sentence. If old_attention_mask, idx, old_input_ids, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:36:25,071 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:36:25,072 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:36:25,072 >>   Batch size = 16\n",
            "01/07/2023 20:36:28 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.536943793296814, 'eval_matthews_correlation': 0.36986609141954047, 'eval_runtime': 3.4463, 'eval_samples_per_second': 302.641, 'eval_steps_per_second': 19.151, 'epoch': 10.0}\n",
            "{'loss': 0.4687, 'learning_rate': 2e-05, 'epoch': 10.28}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:36:59,216 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_attention_mask, idx, old_input_ids, sentence. If old_attention_mask, idx, old_input_ids, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:36:59,217 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:36:59,217 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:36:59,217 >>   Batch size = 16\n",
            "01/07/2023 20:37:02 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5233474969863892, 'eval_matthews_correlation': 0.36261112690943775, 'eval_runtime': 3.4406, 'eval_samples_per_second': 303.142, 'eval_steps_per_second': 19.183, 'epoch': 11.0}\n",
            "{'loss': 0.4768, 'learning_rate': 2e-05, 'epoch': 11.21}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:37:33,439 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_attention_mask, idx, old_input_ids, sentence. If old_attention_mask, idx, old_input_ids, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:37:33,441 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:37:33,441 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:37:33,441 >>   Batch size = 16\n",
            "01/07/2023 20:37:36 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5345401167869568, 'eval_matthews_correlation': 0.37321205597032797, 'eval_runtime': 3.4573, 'eval_samples_per_second': 301.682, 'eval_steps_per_second': 19.09, 'epoch': 12.0}\n",
            "{'loss': 0.459, 'learning_rate': 2e-05, 'epoch': 12.15}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:38:07,690 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_attention_mask, idx, old_input_ids, sentence. If old_attention_mask, idx, old_input_ids, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:38:07,691 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:38:07,691 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:38:07,692 >>   Batch size = 16\n",
            "01/07/2023 20:38:11 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5204418301582336, 'eval_matthews_correlation': 0.34836826417488614, 'eval_runtime': 3.4586, 'eval_samples_per_second': 301.564, 'eval_steps_per_second': 19.083, 'epoch': 13.0}\n",
            "{'loss': 0.4658, 'learning_rate': 2e-05, 'epoch': 13.08}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:38:41,925 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_attention_mask, idx, old_input_ids, sentence. If old_attention_mask, idx, old_input_ids, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:38:41,926 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:38:41,927 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:38:41,927 >>   Batch size = 16\n",
            "01/07/2023 20:38:45 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5350771546363831, 'eval_matthews_correlation': 0.37605227228925864, 'eval_runtime': 3.4832, 'eval_samples_per_second': 299.438, 'eval_steps_per_second': 18.948, 'epoch': 14.0}\n",
            "{'loss': 0.4574, 'learning_rate': 2e-05, 'epoch': 14.02}\n",
            "{'loss': 0.454, 'learning_rate': 2e-05, 'epoch': 14.95}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:39:17,352 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_attention_mask, idx, old_input_ids, sentence. If old_attention_mask, idx, old_input_ids, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:39:17,354 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:39:17,354 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:39:17,354 >>   Batch size = 16\n",
            "01/07/2023 20:39:20 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5249631404876709, 'eval_matthews_correlation': 0.3555838660066059, 'eval_runtime': 3.4491, 'eval_samples_per_second': 302.395, 'eval_steps_per_second': 19.135, 'epoch': 15.0}\n",
            "[INFO|trainer.py:1530] 2023-01-07 20:39:20,803 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 517.4943, 'train_samples_per_second': 247.858, 'train_steps_per_second': 15.507, 'train_loss': 0.4971805087651048, 'epoch': 15.0}\n",
            "[INFO|configuration_utils.py:441] 2023-01-07 20:39:20,804 >> Configuration saved in ../../output/config.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       15.0\n",
            "  train_loss               =     0.4972\n",
            "  train_runtime            = 0:08:37.49\n",
            "  train_samples            =       8551\n",
            "  train_samples_per_second =    247.858\n",
            "  train_steps_per_second   =     15.507\n",
            "01/07/2023 20:39:20 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:566] 2023-01-07 20:39:20,806 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: old_attention_mask, idx, old_input_ids, sentence. If old_attention_mask, idx, old_input_ids, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:39:20,808 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:39:20,808 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:39:20,808 >>   Batch size = 16\n",
            "01/07/2023 20:39:24 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5249631404876709, 'eval_matthews_correlation': 0.3555838660066059, 'eval_runtime': 3.4473, 'eval_samples_per_second': 302.555, 'eval_steps_per_second': 19.145, 'epoch': 15.0}\n",
            "***** eval metrics *****\n",
            "  epoch                     =       15.0\n",
            "  eval_loss                 =      0.525\n",
            "  eval_matthews_correlation =     0.3556\n",
            "  eval_runtime              = 0:00:03.44\n",
            "  eval_samples              =       1043\n",
            "  eval_samples_per_second   =    302.555\n",
            "  eval_steps_per_second     =     19.145\n",
            "01/07/2023 20:39:27 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "01/07/2023 20:39:27 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=True,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=../../output/runs/Jan07_20-39-26_6dbe8a223432,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.CONSTANT,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=15.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=../../output/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=16,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=[],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=../../output/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.NO,\n",
            "save_total_limit=0,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "01/07/2023 20:39:27 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "01/07/2023 20:39:27 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "01/07/2023 20:39:27 - INFO - datasets.info - Loading Dataset info from ../../cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "01/07/2023 20:39:27 - WARNING - datasets.builder - Found cached dataset glue (/content/papa/transformers/papa_scripts/../../cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "01/07/2023 20:39:27 - INFO - datasets.info - Loading Dataset info from /content/papa/transformers/papa_scripts/../../cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "100% 3/3 [00:00<00:00, 884.25it/s]\n",
            "[INFO|configuration_utils.py:654] 2023-01-07 20:39:28,088 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at ../../cache/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:690] 2023-01-07 20:39:28,089 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_seq_length\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sort_calculating\": null,\n",
            "  \"static_heads\": null,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"use_freeze_extract_pooler\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2023-01-07 20:39:28,319 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at ../../cache/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:690] 2023-01-07 20:39:28,320 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_seq_length\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sort_calculating\": null,\n",
            "  \"static_heads\": null,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"use_freeze_extract_pooler\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 20:39:29,007 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at ../../cache/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 20:39:29,007 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at ../../cache/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 20:39:29,007 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 20:39:29,007 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2023-01-07 20:39:29,007 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at ../../cache/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|configuration_utils.py:654] 2023-01-07 20:39:29,120 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at ../../cache/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:690] 2023-01-07 20:39:29,121 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_seq_length\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sort_calculating\": null,\n",
            "  \"static_heads\": null,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"use_freeze_extract_pooler\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1430] 2023-01-07 20:39:29,289 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at ../../cache/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[WARNING|modeling_utils.py:1688] 2023-01-07 20:39:30,905 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'bert.pooler.dense.weight', 'cls.predictions.transform.dense.bias', 'bert.pooler.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1699] 2023-01-07 20:39:30,905 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.5.attention.self.static_heads_content', 'bert.encoder.layer.10.attention.self.static_heads_content', 'bert.encoder.layer.8.attention.self.static_heads_content', 'classifier.bias', 'bert.pooler.weights_per_layer', 'bert.encoder.layer.0.attention.self.static_heads_content', 'bert.encoder.layer.7.attention.self.static_heads_content', 'bert.encoder.layer.1.attention.self.static_heads_content', 'classifier.weight', 'bert.encoder.layer.6.attention.self.static_heads_content', 'bert.encoder.layer.4.attention.self.static_heads_content', 'bert.encoder.layer.9.attention.self.static_heads_content', 'bert.pooler.dense_across_layers.bias', 'bert.pooler.dense_across_layers.weight', 'bert.encoder.layer.3.attention.self.static_heads_content', 'bert.encoder.layer.11.attention.self.static_heads_content', 'bert.encoder.layer.2.attention.self.static_heads_content']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Not frozen param:  bert.pooler.weights_per_layer\n",
            "Not frozen param:  bert.pooler.dense_across_layers.weight\n",
            "Not frozen param:  bert.pooler.dense_across_layers.bias\n",
            "Not frozen param:  classifier.weight\n",
            "Not frozen param:  classifier.bias\n",
            "01/07/2023 20:39:30 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/papa/cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-89b22c6772fe5dc5.arrow\n",
            "01/07/2023 20:39:30 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/papa/cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-9bb9ed57acfe3164.arrow\n",
            "01/07/2023 20:39:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/papa/cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-69b05663891aa8d0.arrow\n",
            "01/07/2023 20:39:31 - INFO - __main__ - Sample 1824 of the training set: {'sentence': 'I acknowledged that my father, he was tight as an owl.', 'label': 0, 'idx': 1824, 'input_ids': [101, 1045, 8969, 2008, 2026, 2269, 1010, 2002, 2001, 4389, 2004, 2019, 13547, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'old_input_ids': [101, 1045, 8969, 2008, 2026, 2269, 1010, 2002, 2001, 4389, 2004, 2019, 13547, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'old_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "01/07/2023 20:39:31 - INFO - __main__ - Sample 409 of the training set: {'sentence': 'For him to do that would be a mistake.', 'label': 1, 'idx': 409, 'input_ids': [101, 2005, 2032, 2000, 2079, 2008, 2052, 2022, 1037, 6707, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'old_input_ids': [101, 2005, 2032, 2000, 2079, 2008, 2052, 2022, 1037, 6707, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'old_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "01/07/2023 20:39:31 - INFO - __main__ - Sample 4506 of the training set: {'sentence': 'Mary sang a song, but Lee never did.', 'label': 1, 'idx': 4506, 'input_ids': [101, 2984, 6369, 1037, 2299, 1010, 2021, 3389, 2196, 2106, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'old_input_ids': [101, 2984, 6369, 1037, 2299, 1010, 2021, 3389, 2196, 2106, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'old_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "run_papa_glue.py:567: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"glue\", data_args.task_name)\n",
            "[INFO|trainer.py:566] 2023-01-07 20:39:33,797 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_input_ids, old_attention_mask, idx. If sentence, old_input_ids, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/envs/env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1290] 2023-01-07 20:39:33,805 >> ***** Running training *****\n",
            "[INFO|trainer.py:1291] 2023-01-07 20:39:33,805 >>   Num examples = 8551\n",
            "[INFO|trainer.py:1292] 2023-01-07 20:39:33,805 >>   Num Epochs = 15\n",
            "[INFO|trainer.py:1293] 2023-01-07 20:39:33,805 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1294] 2023-01-07 20:39:33,805 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1295] 2023-01-07 20:39:33,805 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1296] 2023-01-07 20:39:33,805 >>   Total optimization steps = 8025\n",
            "{'loss': 0.5919, 'learning_rate': 2e-05, 'epoch': 0.93}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:40:04,545 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_input_ids, old_attention_mask, idx. If sentence, old_input_ids, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:40:04,547 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:40:04,547 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:40:04,547 >>   Batch size = 16\n",
            "01/07/2023 20:40:08 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5889080166816711, 'eval_matthews_correlation': 0.058936921430734465, 'eval_runtime': 3.4789, 'eval_samples_per_second': 299.811, 'eval_steps_per_second': 18.972, 'epoch': 1.0}\n",
            "{'loss': 0.5581, 'learning_rate': 2e-05, 'epoch': 1.87}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:40:38,377 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_input_ids, old_attention_mask, idx. If sentence, old_input_ids, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:40:38,378 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:40:38,379 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:40:38,379 >>   Batch size = 16\n",
            "01/07/2023 20:40:41 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.565577507019043, 'eval_matthews_correlation': 0.17981700159532057, 'eval_runtime': 3.3408, 'eval_samples_per_second': 312.198, 'eval_steps_per_second': 19.756, 'epoch': 2.0}\n",
            "{'loss': 0.5499, 'learning_rate': 2e-05, 'epoch': 2.8}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:41:11,760 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_input_ids, old_attention_mask, idx. If sentence, old_input_ids, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:41:11,762 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:41:11,763 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:41:11,763 >>   Batch size = 16\n",
            "01/07/2023 20:41:15 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.565504252910614, 'eval_matthews_correlation': 0.20948530862368064, 'eval_runtime': 3.3916, 'eval_samples_per_second': 307.52, 'eval_steps_per_second': 19.46, 'epoch': 3.0}\n",
            "{'loss': 0.5336, 'learning_rate': 2e-05, 'epoch': 3.74}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:41:45,477 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_input_ids, old_attention_mask, idx. If sentence, old_input_ids, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:41:45,479 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:41:45,479 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:41:45,479 >>   Batch size = 16\n",
            "01/07/2023 20:41:48 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5503054857254028, 'eval_matthews_correlation': 0.27924195152422904, 'eval_runtime': 3.3836, 'eval_samples_per_second': 308.251, 'eval_steps_per_second': 19.506, 'epoch': 4.0}\n",
            "{'loss': 0.5246, 'learning_rate': 2e-05, 'epoch': 4.67}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:42:20,187 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_input_ids, old_attention_mask, idx. If sentence, old_input_ids, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:42:20,189 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:42:20,189 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:42:20,189 >>   Batch size = 16\n",
            "01/07/2023 20:42:23 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5461522936820984, 'eval_matthews_correlation': 0.297019958620285, 'eval_runtime': 3.3728, 'eval_samples_per_second': 309.237, 'eval_steps_per_second': 19.568, 'epoch': 5.0}\n",
            "{'loss': 0.5179, 'learning_rate': 2e-05, 'epoch': 5.61}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:42:53,836 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_input_ids, old_attention_mask, idx. If sentence, old_input_ids, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:42:53,838 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:42:53,838 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:42:53,838 >>   Batch size = 16\n",
            "01/07/2023 20:42:57 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5624563694000244, 'eval_matthews_correlation': 0.29462227671535957, 'eval_runtime': 3.3821, 'eval_samples_per_second': 308.39, 'eval_steps_per_second': 19.515, 'epoch': 6.0}\n",
            "{'loss': 0.51, 'learning_rate': 2e-05, 'epoch': 6.54}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:43:27,349 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_input_ids, old_attention_mask, idx. If sentence, old_input_ids, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:43:27,351 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:43:27,351 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:43:27,351 >>   Batch size = 16\n",
            "01/07/2023 20:43:30 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.569466769695282, 'eval_matthews_correlation': 0.29972796009297104, 'eval_runtime': 3.3603, 'eval_samples_per_second': 310.384, 'eval_steps_per_second': 19.641, 'epoch': 7.0}\n",
            "{'loss': 0.4946, 'learning_rate': 2e-05, 'epoch': 7.48}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:44:00,984 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_input_ids, old_attention_mask, idx. If sentence, old_input_ids, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:44:00,986 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:44:00,986 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:44:00,986 >>   Batch size = 16\n",
            "01/07/2023 20:44:04 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5580621957778931, 'eval_matthews_correlation': 0.31116295404245753, 'eval_runtime': 3.393, 'eval_samples_per_second': 307.394, 'eval_steps_per_second': 19.452, 'epoch': 8.0}\n",
            "{'loss': 0.501, 'learning_rate': 2e-05, 'epoch': 8.41}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:44:34,681 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_input_ids, old_attention_mask, idx. If sentence, old_input_ids, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:44:34,683 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:44:34,683 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:44:34,683 >>   Batch size = 16\n",
            "01/07/2023 20:44:38 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5407667756080627, 'eval_matthews_correlation': 0.36672020645217374, 'eval_runtime': 3.3913, 'eval_samples_per_second': 307.555, 'eval_steps_per_second': 19.462, 'epoch': 9.0}\n",
            "{'loss': 0.488, 'learning_rate': 2e-05, 'epoch': 9.35}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:45:08,330 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_input_ids, old_attention_mask, idx. If sentence, old_input_ids, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:45:08,332 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:45:08,332 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:45:08,332 >>   Batch size = 16\n",
            "01/07/2023 20:45:11 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5526123046875, 'eval_matthews_correlation': 0.33469787134884244, 'eval_runtime': 3.3755, 'eval_samples_per_second': 308.989, 'eval_steps_per_second': 19.553, 'epoch': 10.0}\n",
            "{'loss': 0.4786, 'learning_rate': 2e-05, 'epoch': 10.28}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:45:43,282 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_input_ids, old_attention_mask, idx. If sentence, old_input_ids, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:45:43,284 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:45:43,284 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:45:43,284 >>   Batch size = 16\n",
            "01/07/2023 20:45:46 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5415511727333069, 'eval_matthews_correlation': 0.3549012373122867, 'eval_runtime': 3.3757, 'eval_samples_per_second': 308.977, 'eval_steps_per_second': 19.552, 'epoch': 11.0}\n",
            "{'loss': 0.4882, 'learning_rate': 2e-05, 'epoch': 11.21}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:46:16,801 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_input_ids, old_attention_mask, idx. If sentence, old_input_ids, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:46:16,803 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:46:16,803 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:46:16,803 >>   Batch size = 16\n",
            "01/07/2023 20:46:20 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5530338883399963, 'eval_matthews_correlation': 0.3635828799702745, 'eval_runtime': 3.375, 'eval_samples_per_second': 309.037, 'eval_steps_per_second': 19.556, 'epoch': 12.0}\n",
            "{'loss': 0.47, 'learning_rate': 2e-05, 'epoch': 12.15}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:46:50,333 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_input_ids, old_attention_mask, idx. If sentence, old_input_ids, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:46:50,335 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:46:50,335 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:46:50,335 >>   Batch size = 16\n",
            "01/07/2023 20:46:53 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5361178517341614, 'eval_matthews_correlation': 0.36395023953613853, 'eval_runtime': 3.3737, 'eval_samples_per_second': 309.16, 'eval_steps_per_second': 19.563, 'epoch': 13.0}\n",
            "{'loss': 0.4771, 'learning_rate': 2e-05, 'epoch': 13.08}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:47:23,929 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_input_ids, old_attention_mask, idx. If sentence, old_input_ids, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:47:23,930 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:47:23,930 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:47:23,930 >>   Batch size = 16\n",
            "01/07/2023 20:47:27 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5547087788581848, 'eval_matthews_correlation': 0.3476919731589704, 'eval_runtime': 3.385, 'eval_samples_per_second': 308.123, 'eval_steps_per_second': 19.498, 'epoch': 14.0}\n",
            "{'loss': 0.4673, 'learning_rate': 2e-05, 'epoch': 14.02}\n",
            "{'loss': 0.4643, 'learning_rate': 2e-05, 'epoch': 14.95}\n",
            "[INFO|trainer.py:566] 2023-01-07 20:47:57,543 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_input_ids, old_attention_mask, idx. If sentence, old_input_ids, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:47:57,545 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:47:57,545 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:47:57,545 >>   Batch size = 16\n",
            "01/07/2023 20:48:00 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5408088564872742, 'eval_matthews_correlation': 0.3555838660066059, 'eval_runtime': 3.3811, 'eval_samples_per_second': 308.479, 'eval_steps_per_second': 19.52, 'epoch': 15.0}\n",
            "[INFO|trainer.py:1530] 2023-01-07 20:48:00,926 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 507.1207, 'train_samples_per_second': 252.928, 'train_steps_per_second': 15.825, 'train_loss': 0.5070186870491764, 'epoch': 15.0}\n",
            "[INFO|configuration_utils.py:441] 2023-01-07 20:48:00,927 >> Configuration saved in ../../output/config.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       15.0\n",
            "  train_loss               =      0.507\n",
            "  train_runtime            = 0:08:27.12\n",
            "  train_samples            =       8551\n",
            "  train_samples_per_second =    252.928\n",
            "  train_steps_per_second   =     15.825\n",
            "01/07/2023 20:48:00 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:566] 2023-01-07 20:48:00,929 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_input_ids, old_attention_mask, idx. If sentence, old_input_ids, old_attention_mask, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2023-01-07 20:48:00,931 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2023-01-07 20:48:00,931 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2023-01-07 20:48:00,931 >>   Batch size = 16\n",
            "01/07/2023 20:48:04 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5408088564872742, 'eval_matthews_correlation': 0.3555838660066059, 'eval_runtime': 3.3754, 'eval_samples_per_second': 308.999, 'eval_steps_per_second': 19.553, 'epoch': 15.0}\n",
            "***** eval metrics *****\n",
            "  epoch                     =       15.0\n",
            "  eval_loss                 =     0.5408\n",
            "  eval_matthews_correlation =     0.3556\n",
            "  eval_runtime              = 0:00:03.37\n",
            "  eval_samples              =       1043\n",
            "  eval_samples_per_second   =    308.999\n",
            "  eval_steps_per_second     =     19.553\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls papa/outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGNQVD_Vw9n-",
        "outputId": "0084be9d-47cb-4a03-b7bd-b95a3749d189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention_patterns_sums.pt  attention_sums.pt  avgs.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "source activate env\n",
        "python\n",
        "\n",
        "import torch\n",
        "attention_sums = torch.load('papa/outputs/attention_sums.pt')\n",
        "print(f\"Shape of attention_sums : {attention_sums.size()}\")         # (num_layers=12, num_heads=12, )\n",
        "\n",
        "print(attention_sums[0][0].min())\n",
        "# DIMENSIONS :\n",
        "\n",
        "# num_layers = 12\n",
        "# num_heads = 12\n",
        "# num_input_tokens (n) = 64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KD6Ocg8VwZPv",
        "outputId": "90dd9747-6a82-424c-b9a1-e123c3a98697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of attention_sums : torch.Size([12, 12, 64, 64])\n",
            "tensor(0.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7aJIfQjrPB9l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}