{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5a2omcHjOzM",
        "outputId": "ab5bb4e2-5f04-4b78-abd4-619f1e693717"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏬ Downloading https://github.com/jaimergp/miniforge/releases/latest/download/Mambaforge-colab-Linux-x86_64.sh...\n",
            "📦 Installing...\n",
            "📌 Adjusting configuration...\n",
            "🩹 Patching environment...\n",
            "⏲ Done in 0:00:15\n",
            "🔁 Restarting kernel...\n"
          ]
        }
      ],
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "conda create --name env python=3.8 -y\n",
        "source activate env\n",
        "conda install pytorch -c pytorch-lts -c nvidia\n",
        "\n",
        "python -m pip install -U pip\n",
        "pip install --upgrade setuptools\n",
        "\n",
        "# using an editable install for now\n",
        "git clone https://github.com/huggingface/transformers.git\n",
        "cd transformers\n",
        "pip install -e .\n",
        "\n",
        "pip install datasets accelerate scipy scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLtrMTS-jfnF",
        "outputId": "06452979-34a7-4527-d147-d460830ec07f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "  current version: 22.9.0\n",
            "  latest version: 22.11.1\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/env\n",
            "\n",
            "  added / updated specs:\n",
            "    - python=3.8\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    ca-certificates-2022.12.7  |       ha878542_0         143 KB  conda-forge\n",
            "    setuptools-65.6.3          |     pyhd8ed1ab_0         619 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         761 KB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge None\n",
            "  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu None\n",
            "  bzip2              conda-forge/linux-64::bzip2-1.0.8-h7f98852_4 None\n",
            "  ca-certificates    conda-forge/linux-64::ca-certificates-2022.12.7-ha878542_0 None\n",
            "  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.39-hcc3a1bd_1 None\n",
            "  libffi             conda-forge/linux-64::libffi-3.4.2-h7f98852_5 None\n",
            "  libgcc-ng          conda-forge/linux-64::libgcc-ng-12.2.0-h65d4601_19 None\n",
            "  libgomp            conda-forge/linux-64::libgomp-12.2.0-h65d4601_19 None\n",
            "  libnsl             conda-forge/linux-64::libnsl-2.0.0-h7f98852_0 None\n",
            "  libsqlite          conda-forge/linux-64::libsqlite-3.40.0-h753d276_0 None\n",
            "  libuuid            conda-forge/linux-64::libuuid-2.32.1-h7f98852_1000 None\n",
            "  libzlib            conda-forge/linux-64::libzlib-1.2.13-h166bdaf_4 None\n",
            "  ncurses            conda-forge/linux-64::ncurses-6.3-h27087fc_1 None\n",
            "  openssl            conda-forge/linux-64::openssl-3.0.7-h0b41bf4_1 None\n",
            "  pip                conda-forge/noarch::pip-22.3.1-pyhd8ed1ab_0 None\n",
            "  python             conda-forge/linux-64::python-3.8.15-h4a9ceb5_0_cpython None\n",
            "  readline           conda-forge/linux-64::readline-8.1.2-h0f457ee_0 None\n",
            "  setuptools         conda-forge/noarch::setuptools-65.6.3-pyhd8ed1ab_0 None\n",
            "  tk                 conda-forge/linux-64::tk-8.6.12-h27826a3_0 None\n",
            "  wheel              conda-forge/noarch::wheel-0.38.4-pyhd8ed1ab_0 None\n",
            "  xz                 conda-forge/linux-64::xz-5.2.6-h166bdaf_0 None\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "ca-certificates-2022 | 143 KB    | : 100% 1.0/1 [00:00<00:00,  7.31it/s]                \n",
            "setuptools-65.6.3    | 619 KB    | : 100% 1.0/1 [00:00<00:00,  6.74it/s]\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate env\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n",
            "Retrieving notices: ...working... done\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "  current version: 22.9.0\n",
            "  latest version: 22.11.1\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/env\n",
            "\n",
            "  added / updated specs:\n",
            "    - pytorch\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    _openmp_mutex-4.5          |       2_kmp_llvm           6 KB  conda-forge\n",
            "    blas-2.106                 |              mkl          12 KB  conda-forge\n",
            "    cudatoolkit-11.1.74        |       h6bb024c_0        1.19 GB  nvidia\n",
            "    libblas-3.9.0              |            6_mkl          11 KB  conda-forge\n",
            "    libcblas-3.9.0             |            6_mkl          11 KB  conda-forge\n",
            "    libgfortran-ng-12.2.0      |      h69a702a_19          22 KB  conda-forge\n",
            "    libgfortran5-12.2.0        |      h337968e_19         1.8 MB  conda-forge\n",
            "    liblapack-3.9.0            |            6_mkl          11 KB  conda-forge\n",
            "    liblapacke-3.9.0           |            6_mkl          11 KB  conda-forge\n",
            "    libuv-1.44.2               |       h166bdaf_0         1.0 MB  conda-forge\n",
            "    llvm-openmp-15.0.6         |       he0ac6c6_0         3.3 MB  conda-forge\n",
            "    mkl-2020.4                 |     h726a3e6_304       215.6 MB  conda-forge\n",
            "    ninja-1.11.0               |       h924138e_0         2.8 MB  conda-forge\n",
            "    numpy-1.24.0               |   py38hab0fcb9_0         6.3 MB  conda-forge\n",
            "    pytorch-1.8.2              |py3.8_cuda11.1_cudnn8.0.5_0        1.27 GB  pytorch-lts\n",
            "    typing_extensions-4.4.0    |     pyha770c72_0          29 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        2.68 GB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  blas               conda-forge/linux-64::blas-2.106-mkl None\n",
            "  cudatoolkit        nvidia/linux-64::cudatoolkit-11.1.74-h6bb024c_0 None\n",
            "  libblas            conda-forge/linux-64::libblas-3.9.0-6_mkl None\n",
            "  libcblas           conda-forge/linux-64::libcblas-3.9.0-6_mkl None\n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-12.2.0-h69a702a_19 None\n",
            "  libgfortran5       conda-forge/linux-64::libgfortran5-12.2.0-h337968e_19 None\n",
            "  liblapack          conda-forge/linux-64::liblapack-3.9.0-6_mkl None\n",
            "  liblapacke         conda-forge/linux-64::liblapacke-3.9.0-6_mkl None\n",
            "  libstdcxx-ng       conda-forge/linux-64::libstdcxx-ng-12.2.0-h46fd767_19 None\n",
            "  libuv              conda-forge/linux-64::libuv-1.44.2-h166bdaf_0 None\n",
            "  llvm-openmp        conda-forge/linux-64::llvm-openmp-15.0.6-he0ac6c6_0 None\n",
            "  mkl                conda-forge/linux-64::mkl-2020.4-h726a3e6_304 None\n",
            "  ninja              conda-forge/linux-64::ninja-1.11.0-h924138e_0 None\n",
            "  numpy              conda-forge/linux-64::numpy-1.24.0-py38hab0fcb9_0 None\n",
            "  python_abi         conda-forge/linux-64::python_abi-3.8-3_cp38 None\n",
            "  pytorch            pytorch-lts/linux-64::pytorch-1.8.2-py3.8_cuda11.1_cudnn8.0.5_0 None\n",
            "  typing_extensions  conda-forge/noarch::typing_extensions-4.4.0-pyha770c72_0 None\n",
            "\n",
            "The following packages will be DOWNGRADED:\n",
            "\n",
            "  _openmp_mutex                                   4.5-2_gnu --> 4.5-2_kmp_llvm None\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "typing_extensions-4. | 29 KB     | : 100% 1.0/1 [00:00<00:00,  8.28it/s]               \n",
            "liblapacke-3.9.0     | 11 KB     | : 100% 1.0/1 [00:00<00:00, 12.40it/s]\n",
            "blas-2.106           | 12 KB     | : 100% 1.0/1 [00:00<00:00, 20.79it/s]\n",
            "ninja-1.11.0         | 2.8 MB    | : 100% 1.0/1 [00:00<00:00,  2.18it/s]               \n",
            "libcblas-3.9.0       | 11 KB     | : 100% 1.0/1 [00:00<00:00, 22.25it/s]\n",
            "libgfortran-ng-12.2. | 22 KB     | : 100% 1.0/1 [00:00<00:00, 20.74it/s]\n",
            "mkl-2020.4           | 215.6 MB  | : 100% 1.0/1 [00:34<00:00, 34.28s/it]               \n",
            "pytorch-1.8.2        | 1.27 GB   | : 100% 1.0/1 [03:03<00:00, 183.25s/it]\n",
            "liblapack-3.9.0      | 11 KB     | : 100% 1.0/1 [00:00<00:00, 13.18it/s]\n",
            "numpy-1.24.0         | 6.3 MB    | : 100% 1.0/1 [00:00<00:00,  1.58it/s]\n",
            "libuv-1.44.2         | 1.0 MB    | : 100% 1.0/1 [00:00<00:00,  5.47it/s]\n",
            "llvm-openmp-15.0.6   | 3.3 MB    | : 100% 1.0/1 [00:00<00:00,  6.15it/s]\n",
            "libblas-3.9.0        | 11 KB     | : 100% 1.0/1 [00:00<00:00, 10.67it/s]\n",
            "libgfortran5-12.2.0  | 1.8 MB    | : 100% 1.0/1 [00:00<00:00,  3.13it/s]\n",
            "cudatoolkit-11.1.74  | 1.19 GB   | : 100% 1.0/1 [02:22<00:00, 142.43s/it]              \n",
            "_openmp_mutex-4.5    | 6 KB      | : 100% 1.0/1 [00:00<00:00, 18.69it/s]\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n",
            "\n",
            "\b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Retrieving notices: ...working... done\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/envs/env/lib/python3.8/site-packages (22.3.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: setuptools in /usr/local/envs/env/lib/python3.8/site-packages (65.6.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCloning into 'transformers'...\n",
            "remote: Enumerating objects: 120640, done.\u001b[K\n",
            "remote: Counting objects: 100% (74/74), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 120640 (delta 21), reused 36 (delta 5), pack-reused 120566\u001b[K\n",
            "Receiving objects: 100% (120640/120640), 114.15 MiB | 30.64 MiB/s, done.\n",
            "Resolving deltas: 100% (90154/90154), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting requests\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting regex!=2019.12.17\n",
            "  Downloading regex-2022.10.31-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.3/772.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m701.2/701.2 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/envs/env/lib/python3.8/site-packages (from transformers==4.26.0.dev0) (1.24.0)\n",
            "Collecting filelock\n",
            "  Downloading filelock-3.8.2-py3-none-any.whl (10 kB)\n",
            "Collecting packaging>=20.0\n",
            "  Downloading packaging-22.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm>=4.27\n",
            "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/envs/env/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.26.0.dev0) (4.4.0)\n",
            "Collecting charset-normalizer<3,>=2\n",
            "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna<4,>=2.5\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: transformers\n",
            "  Building editable for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.26.0.dev0-0.editable-py3-none-any.whl size=32356 sha256=e6dac8d30954c9cf771855db697d98b3b1ba1a0776e4cbcb6b591e6bd141fe1d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-x1uy9pk0/wheels/15/57/14/0d2873a0295966ca166ea9d9225761a50cce27e4d6b0341fcc\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, urllib3, tqdm, regex, pyyaml, packaging, idna, filelock, charset-normalizer, certifi, requests, huggingface-hub, transformers\n",
            "Successfully installed certifi-2022.12.7 charset-normalizer-2.1.1 filelock-3.8.2 huggingface-hub-0.11.1 idna-3.4 packaging-22.0 pyyaml-6.0 regex-2022.10.31 requests-2.28.1 tokenizers-0.13.2 tqdm-4.64.1 transformers-4.26.0.dev0 urllib3-1.26.13\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "source activate env\n",
        "git clone https://github.com/schwartz-lab-NLP/papa.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8HUnX1pniek",
        "outputId": "8afb5fb1-7028-4f13-8fb6-630e417de6a0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'papa'...\n",
            "remote: Enumerating objects: 120, done.\u001b[K\n",
            "remote: Counting objects: 100% (120/120), done.\u001b[K\n",
            "remote: Compressing objects: 100% (96/96), done.\u001b[K\n",
            "remote: Total 120 (delta 40), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (120/120), 138.92 KiB | 2.89 MiB/s, done.\n",
            "Resolving deltas: 100% (40/40), done.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "source activate env\n",
        "pip uninstall transformers -y\n",
        "pip install transformers==4.18.0\n",
        "pip show transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LWhBX2jECGZ",
        "outputId": "7a1990e4-00b1-48df-e8de-4edde9494e87"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.20.1\n",
            "Uninstalling transformers-4.20.1:\n",
            "  Successfully uninstalled transformers-4.20.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.18.0\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/envs/env/lib/python3.8/site-packages (from transformers==4.18.0) (0.11.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/envs/env/lib/python3.8/site-packages (from transformers==4.18.0) (1.24.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/envs/env/lib/python3.8/site-packages (from transformers==4.18.0) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/envs/env/lib/python3.8/site-packages (from transformers==4.18.0) (3.8.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/envs/env/lib/python3.8/site-packages (from transformers==4.18.0) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/envs/env/lib/python3.8/site-packages (from transformers==4.18.0) (22.0)\n",
            "Requirement already satisfied: requests in /usr/local/envs/env/lib/python3.8/site-packages (from transformers==4.18.0) (2.28.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/envs/env/lib/python3.8/site-packages (from transformers==4.18.0) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/envs/env/lib/python3.8/site-packages (from transformers==4.18.0) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/envs/env/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/envs/env/lib/python3.8/site-packages (from requests->transformers==4.18.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/envs/env/lib/python3.8/site-packages (from requests->transformers==4.18.0) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/envs/env/lib/python3.8/site-packages (from requests->transformers==4.18.0) (1.26.13)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/envs/env/lib/python3.8/site-packages (from requests->transformers==4.18.0) (3.4)\n",
            "Requirement already satisfied: six in /usr/local/envs/env/lib/python3.8/site-packages (from sacremoses->transformers==4.18.0) (1.16.0)\n",
            "Collecting click\n",
            "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/envs/env/lib/python3.8/site-packages (from sacremoses->transformers==4.18.0) (1.2.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=ccbfd4c77736572a57756f3afe7b680f64a3f0bd1f7c7dd1b82790c9ac5d5737\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/a3/ff/01dc060d7fc51176b3ce7cf1561466a12e658164b594747547\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: click, sacremoses, transformers\n",
            "Successfully installed click-8.1.3 sacremoses-0.0.53 transformers-4.18.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mName: transformers\n",
            "Version: 4.18.0\n",
            "Summary: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch\n",
            "Home-page: https://github.com/huggingface/transformers\n",
            "Author: Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Sam Shleifer, Patrick von Platen, Sylvain Gugger, Suraj Patil, Stas Bekman, Google AI Language Team Authors, Open AI team Authors, Facebook AI Authors, Carnegie Mellon University Authors\n",
            "Author-email: thomas@huggingface.co\n",
            "License: Apache\n",
            "Location: /usr/local/envs/env/lib/python3.8/site-packages\n",
            "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, sacremoses, tokenizers, tqdm\n",
            "Required-by: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pycat /usr/local/envs/env/lib/python3.8/site-packages/transformers/file_utils.py"
      ],
      "metadata": {
        "id": "nvbkNN3iCVD5"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls papa/transformers/src/transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKNu3TnbBfiN",
        "outputId": "d0fe56f4-a955-4b6a-8494-ef588dc73cbe"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "modeling_utils.py  models  papa_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy local transformers content to installed transformers location\n",
        "!cp -R papa/transformers/src/transformers/* /usr/local/envs/env/lib/python3.8/site-packages/transformers/"
      ],
      "metadata": {
        "id": "WOu5NBYqnDS_"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "source activate env\n",
        "pip uninstall transformers -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3EQDGpzDt1q",
        "outputId": "fcc8a0ea-9ad2-4568-9f2f-6e531ec1f6d2"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.26.0.dev0\n",
            "Uninstalling transformers-4.26.0.dev0:\n",
            "  Would remove:\n",
            "    /usr/local/envs/env/bin/transformers-cli\n",
            "    /usr/local/envs/env/lib/python3.8/site-packages/__editable__.transformers-4.26.0.dev0.pth\n",
            "    /usr/local/envs/env/lib/python3.8/site-packages/transformers-4.26.0.dev0.dist-info/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled transformers-4.26.0.dev0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "source activate env\n",
        "cd papa/transformers/papa_scripts\n",
        "\n",
        "MODEL=bert-base-uncased\n",
        "TASK=cola\n",
        "\n",
        "python3 run_papa_glue_avgs_creator.py --model_name_or_path ${MODEL}  --task_name ${TASK}  --max_length 64    --per_device_train_batch_size 8   --output_dir ../../constant-matrices/ --cache_dir ../../cache/ --use_papa_preprocess true  --pad_to_max_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMBMB-MZn_iz",
        "outputId": "103c7224-9daf-44e1-8b58-ea9eaa56e3f6"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/27/2022 17:16:05 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "Mixed precision type: no\n",
            "\n",
            "12/27/2022 17:16:06 - WARNING - datasets.builder - Found cached dataset glue (/content/papa/transformers/papa_scripts/../../cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "100% 3/3 [00:00<00:00, 862.14it/s]\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpfffa41bh\n",
            "Downloading: 100% 570/570 [00:00<00:00, 615kB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_seq_length\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sort_calculating\": null,\n",
            "  \"static_heads\": null,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"use_freeze_extract_pooler\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp7_jxcxm7\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 37.8kB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_seq_length\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sort_calculating\": null,\n",
            "  \"static_heads\": null,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"use_freeze_extract_pooler\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp0h5su2rk\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 2.20MB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp4plmsv78\n",
            "Downloading: 100% 455k/455k [00:00<00:00, 3.27MB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_seq_length\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sort_calculating\": null,\n",
            "  \"static_heads\": null,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"use_freeze_extract_pooler\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at ../../cache/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset: 100% 9/9 [00:00<00:00, 13.79ba/s]\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00, 32.33ba/s]\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00, 28.92ba/s]\n",
            "12/27/2022 17:16:11 - INFO - __main__ - Sample 2471 of the training set: {'input_ids': [101, 4532, 6369, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'labels': 1}.\n",
            "12/27/2022 17:16:11 - INFO - __main__ - Sample 1865 of the training set: {'input_ids': [101, 1045, 3764, 2000, 2032, 2055, 1996, 2162, 7483, 1010, 2008, 3124, 2040, 1005, 1055, 2467, 2206, 2149, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'labels': 1}.\n",
            "12/27/2022 17:16:11 - INFO - __main__ - Sample 6911 of the training set: {'input_ids': [101, 3958, 2001, 24935, 24494, 4691, 15664, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'labels': 1}.\n",
            "/usr/local/envs/env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "run_papa_glue_avgs_creator.py:452: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"glue\", args.task_name)\n",
            "12/27/2022 17:16:14 - INFO - __main__ - ***** Running training *****\n",
            "12/27/2022 17:16:14 - INFO - __main__ -   Num examples = 8551\n",
            "12/27/2022 17:16:14 - INFO - __main__ -   Num Epochs = 3\n",
            "12/27/2022 17:16:14 - INFO - __main__ -   Instantaneous batch size per device = 8\n",
            "12/27/2022 17:16:14 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "12/27/2022 17:16:14 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
            "12/27/2022 17:16:14 - INFO - __main__ -   Total optimization steps = 3207\n",
            "  0% 0/3207 [00:34<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "source activate env\n",
        "cd papa/transformers/papa_scripts\n",
        "\n",
        "MODEL=bert-base-uncased\n",
        "TASK=cola\n",
        "\n",
        "python3 run_papa_glue.py --model_name_or_path ${MODEL} --task_name ${TASK} --do_eval --max_seq_length 64 --per_device_train_batch_size 16 --per_device_eval_batch_size 16 --output_dir ../../sorted-heads/ --cache_dir ../../cache/  --do_train --num_train_epochs 15.0 --learning_rate 2e-5 --lr_scheduler_type constant --disable_tqdm true --evaluation_strategy epoch --save_strategy no --use_papa_preprocess --use_freeze_extract_pooler true --static_heads_dir ../../constant-matrices/  --save_total_limit 0 --sort_calculating True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3Dmx5bsl1o0",
        "outputId": "5ee9bffb-b7c7-4fa7-b9a7-70b4a9396feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/27/2022 17:19:47 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "12/27/2022 17:19:47 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=True,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=../../sorted-heads/runs/Dec27_17-19-46_850b55d57750,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.CONSTANT,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=15.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=../../sorted-heads/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=16,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=[],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=../../sorted-heads/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.NO,\n",
            "save_total_limit=0,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "12/27/2022 17:19:48 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "12/27/2022 17:19:48 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "12/27/2022 17:19:48 - INFO - datasets.info - Loading Dataset info from ../../cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "12/27/2022 17:19:48 - WARNING - datasets.builder - Found cached dataset glue (/content/papa/transformers/papa_scripts/../../cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "12/27/2022 17:19:48 - INFO - datasets.info - Loading Dataset info from /content/papa/transformers/papa_scripts/../../cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "100% 3/3 [00:00<00:00, 888.56it/s]\n",
            "[INFO|configuration_utils.py:654] 2022-12-27 17:19:48,555 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at ../../cache/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:690] 2022-12-27 17:19:48,556 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_seq_length\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sort_calculating\": null,\n",
            "  \"static_heads\": null,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"use_freeze_extract_pooler\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2022-12-27 17:19:48,856 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at ../../cache/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:690] 2022-12-27 17:19:48,856 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_seq_length\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sort_calculating\": null,\n",
            "  \"static_heads\": null,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"use_freeze_extract_pooler\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-12-27 17:19:49,753 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at ../../cache/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-12-27 17:19:49,753 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at ../../cache/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-12-27 17:19:49,753 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-12-27 17:19:49,753 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-12-27 17:19:49,753 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at ../../cache/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|configuration_utils.py:654] 2022-12-27 17:19:49,912 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at ../../cache/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:690] 2022-12-27 17:19:49,913 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_seq_length\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sort_calculating\": null,\n",
            "  \"static_heads\": null,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"use_freeze_extract_pooler\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1430] 2022-12-27 17:19:50,120 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at ../../cache/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[WARNING|modeling_utils.py:1688] 2022-12-27 17:19:51,782 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'bert.pooler.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1699] 2022-12-27 17:19:51,782 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.10.attention.self.static_heads_content', 'bert.encoder.layer.11.attention.self.static_heads_content', 'bert.encoder.layer.0.attention.self.lambdas', 'bert.encoder.layer.3.attention.self.static_heads_content', 'bert.encoder.layer.9.attention.self.lambdas', 'bert.encoder.layer.6.attention.self.static_heads_content', 'bert.encoder.layer.6.attention.self.lambdas', 'bert.encoder.layer.5.attention.self.lambdas', 'bert.encoder.layer.4.attention.self.lambdas', 'bert.encoder.layer.1.attention.self.static_heads_content', 'bert.encoder.layer.5.attention.self.static_heads_content', 'bert.pooler.dense_across_layers.weight', 'bert.encoder.layer.2.attention.self.lambdas', 'classifier.bias', 'bert.encoder.layer.0.attention.self.static_heads_content', 'bert.encoder.layer.8.attention.self.lambdas', 'bert.encoder.layer.7.attention.self.static_heads_content', 'bert.encoder.layer.10.attention.self.lambdas', 'bert.encoder.layer.9.attention.self.static_heads_content', 'classifier.weight', 'bert.encoder.layer.8.attention.self.static_heads_content', 'bert.pooler.dense_across_layers.bias', 'bert.encoder.layer.3.attention.self.lambdas', 'bert.encoder.layer.4.attention.self.static_heads_content', 'bert.encoder.layer.2.attention.self.static_heads_content', 'bert.encoder.layer.7.attention.self.lambdas', 'bert.encoder.layer.1.attention.self.lambdas', 'bert.pooler.weights_per_layer', 'bert.encoder.layer.11.attention.self.lambdas']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Not frozen param:  bert.encoder.layer.0.attention.self.lambdas\n",
            "Not frozen param:  bert.encoder.layer.1.attention.self.lambdas\n",
            "Not frozen param:  bert.encoder.layer.2.attention.self.lambdas\n",
            "Not frozen param:  bert.encoder.layer.3.attention.self.lambdas\n",
            "Not frozen param:  bert.encoder.layer.4.attention.self.lambdas\n",
            "Not frozen param:  bert.encoder.layer.5.attention.self.lambdas\n",
            "Not frozen param:  bert.encoder.layer.6.attention.self.lambdas\n",
            "Not frozen param:  bert.encoder.layer.7.attention.self.lambdas\n",
            "Not frozen param:  bert.encoder.layer.8.attention.self.lambdas\n",
            "Not frozen param:  bert.encoder.layer.9.attention.self.lambdas\n",
            "Not frozen param:  bert.encoder.layer.10.attention.self.lambdas\n",
            "Not frozen param:  bert.encoder.layer.11.attention.self.lambdas\n",
            "Not frozen param:  bert.pooler.weights_per_layer\n",
            "Not frozen param:  bert.pooler.dense_across_layers.weight\n",
            "Not frozen param:  bert.pooler.dense_across_layers.bias\n",
            "Not frozen param:  classifier.weight\n",
            "Not frozen param:  classifier.bias\n",
            "Running tokenizer on dataset:   0% 0/9 [00:00<?, ?ba/s]12/27/2022 17:19:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/papa/cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-89b22c6772fe5dc5.arrow\n",
            "Running tokenizer on dataset: 100% 9/9 [00:00<00:00, 13.08ba/s]\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]12/27/2022 17:19:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/papa/cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-101b8c27ae391d4f.arrow\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00, 22.67ba/s]\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]12/27/2022 17:19:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/papa/cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2d744ddb657954a1.arrow\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00, 25.73ba/s]\n",
            "12/27/2022 17:19:52 - INFO - __main__ - Sample 1824 of the training set: {'sentence': 'I acknowledged that my father, he was tight as an owl.', 'label': 0, 'idx': 1824, 'input_ids': [101, 1045, 8969, 2008, 2026, 2269, 1010, 2002, 2001, 4389, 2004, 2019, 13547, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'old_input_ids': [101, 1045, 8969, 2008, 2026, 2269, 1010, 2002, 2001, 4389, 2004, 2019, 13547, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'old_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "12/27/2022 17:19:52 - INFO - __main__ - Sample 409 of the training set: {'sentence': 'For him to do that would be a mistake.', 'label': 1, 'idx': 409, 'input_ids': [101, 2005, 2032, 2000, 2079, 2008, 2052, 2022, 1037, 6707, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'old_input_ids': [101, 2005, 2032, 2000, 2079, 2008, 2052, 2022, 1037, 6707, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'old_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "12/27/2022 17:19:52 - INFO - __main__ - Sample 4506 of the training set: {'sentence': 'Mary sang a song, but Lee never did.', 'label': 1, 'idx': 4506, 'input_ids': [101, 2984, 6369, 1037, 2299, 1010, 2021, 3389, 2196, 2106, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 'old_input_ids': [101, 2984, 6369, 1037, 2299, 1010, 2021, 3389, 2196, 2106, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'old_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "run_papa_glue.py:567: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"glue\", data_args.task_name)\n",
            "[INFO|trainer.py:566] 2022-12-27 17:19:55,563 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_attention_mask, old_input_ids, idx. If sentence, old_attention_mask, old_input_ids, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/envs/env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1290] 2022-12-27 17:19:55,571 >> ***** Running training *****\n",
            "[INFO|trainer.py:1291] 2022-12-27 17:19:55,571 >>   Num examples = 8551\n",
            "[INFO|trainer.py:1292] 2022-12-27 17:19:55,571 >>   Num Epochs = 15\n",
            "[INFO|trainer.py:1293] 2022-12-27 17:19:55,571 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1294] 2022-12-27 17:19:55,571 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1295] 2022-12-27 17:19:55,571 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1296] 2022-12-27 17:19:55,571 >>   Total optimization steps = 8025\n",
            "{'loss': 0.5746, 'learning_rate': 2e-05, 'epoch': 0.93}\n",
            "[INFO|trainer.py:566] 2022-12-27 17:21:00,110 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_attention_mask, old_input_ids, idx. If sentence, old_attention_mask, old_input_ids, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2022-12-27 17:21:00,111 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2022-12-27 17:21:00,112 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2022-12-27 17:21:00,112 >>   Batch size = 16\n",
            "12/27/2022 17:21:04 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5603216886520386, 'eval_matthews_correlation': 0.22781131848083777, 'eval_runtime': 4.0389, 'eval_samples_per_second': 258.236, 'eval_steps_per_second': 16.341, 'epoch': 1.0}\n",
            "{'loss': 0.519, 'learning_rate': 2e-05, 'epoch': 1.87}\n",
            "[INFO|trainer.py:566] 2022-12-27 17:22:07,421 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_attention_mask, old_input_ids, idx. If sentence, old_attention_mask, old_input_ids, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2022-12-27 17:22:07,422 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2022-12-27 17:22:07,422 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2022-12-27 17:22:07,423 >>   Batch size = 16\n",
            "12/27/2022 17:22:11 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5284054279327393, 'eval_matthews_correlation': 0.3046308372846143, 'eval_runtime': 4.0064, 'eval_samples_per_second': 260.33, 'eval_steps_per_second': 16.473, 'epoch': 2.0}\n",
            "{'loss': 0.505, 'learning_rate': 2e-05, 'epoch': 2.8}\n",
            "[INFO|trainer.py:566] 2022-12-27 17:23:14,909 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_attention_mask, old_input_ids, idx. If sentence, old_attention_mask, old_input_ids, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2022-12-27 17:23:14,911 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2022-12-27 17:23:14,911 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2022-12-27 17:23:14,911 >>   Batch size = 16\n",
            "12/27/2022 17:23:18 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5299597978591919, 'eval_matthews_correlation': 0.3222180119516509, 'eval_runtime': 3.9879, 'eval_samples_per_second': 261.542, 'eval_steps_per_second': 16.55, 'epoch': 3.0}\n",
            "{'loss': 0.4882, 'learning_rate': 2e-05, 'epoch': 3.74}\n",
            "[INFO|trainer.py:566] 2022-12-27 17:24:22,990 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_attention_mask, old_input_ids, idx. If sentence, old_attention_mask, old_input_ids, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2022-12-27 17:24:22,992 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2022-12-27 17:24:22,992 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2022-12-27 17:24:22,992 >>   Batch size = 16\n",
            "12/27/2022 17:24:27 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5088865756988525, 'eval_matthews_correlation': 0.3702568889845357, 'eval_runtime': 4.0262, 'eval_samples_per_second': 259.053, 'eval_steps_per_second': 16.393, 'epoch': 4.0}\n",
            "{'loss': 0.4822, 'learning_rate': 2e-05, 'epoch': 4.67}\n",
            "[INFO|trainer.py:566] 2022-12-27 17:25:30,434 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_attention_mask, old_input_ids, idx. If sentence, old_attention_mask, old_input_ids, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2022-12-27 17:25:30,436 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2022-12-27 17:25:30,436 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2022-12-27 17:25:30,436 >>   Batch size = 16\n",
            "12/27/2022 17:25:34 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5019650459289551, 'eval_matthews_correlation': 0.36986609141954047, 'eval_runtime': 4.0033, 'eval_samples_per_second': 260.534, 'eval_steps_per_second': 16.486, 'epoch': 5.0}\n",
            "{'loss': 0.4737, 'learning_rate': 2e-05, 'epoch': 5.61}\n",
            "[INFO|trainer.py:566] 2022-12-27 17:26:37,856 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_attention_mask, old_input_ids, idx. If sentence, old_attention_mask, old_input_ids, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2022-12-27 17:26:37,858 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2022-12-27 17:26:37,858 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2022-12-27 17:26:37,858 >>   Batch size = 16\n",
            "12/27/2022 17:26:41 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.547709584236145, 'eval_matthews_correlation': 0.3818070077211945, 'eval_runtime': 4.0085, 'eval_samples_per_second': 260.2, 'eval_steps_per_second': 16.465, 'epoch': 6.0}\n",
            "{'loss': 0.4652, 'learning_rate': 2e-05, 'epoch': 6.54}\n",
            "[INFO|trainer.py:566] 2022-12-27 17:27:45,867 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_attention_mask, old_input_ids, idx. If sentence, old_attention_mask, old_input_ids, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2022-12-27 17:27:45,869 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2022-12-27 17:27:45,869 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2022-12-27 17:27:45,869 >>   Batch size = 16\n",
            "12/27/2022 17:27:49 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5359765887260437, 'eval_matthews_correlation': 0.39302533664823136, 'eval_runtime': 4.0271, 'eval_samples_per_second': 258.995, 'eval_steps_per_second': 16.389, 'epoch': 7.0}\n",
            "{'loss': 0.4498, 'learning_rate': 2e-05, 'epoch': 7.48}\n",
            "[INFO|trainer.py:566] 2022-12-27 17:28:53,363 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_attention_mask, old_input_ids, idx. If sentence, old_attention_mask, old_input_ids, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2022-12-27 17:28:53,365 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2022-12-27 17:28:53,365 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2022-12-27 17:28:53,365 >>   Batch size = 16\n",
            "12/27/2022 17:28:57 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5259395837783813, 'eval_matthews_correlation': 0.39216697445681503, 'eval_runtime': 4.024, 'eval_samples_per_second': 259.193, 'eval_steps_per_second': 16.401, 'epoch': 8.0}\n",
            "{'loss': 0.4527, 'learning_rate': 2e-05, 'epoch': 8.41}\n",
            "[INFO|trainer.py:566] 2022-12-27 17:30:00,796 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_attention_mask, old_input_ids, idx. If sentence, old_attention_mask, old_input_ids, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2022-12-27 17:30:00,798 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2022-12-27 17:30:00,798 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2022-12-27 17:30:00,798 >>   Batch size = 16\n",
            "12/27/2022 17:30:04 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5107746720314026, 'eval_matthews_correlation': 0.42425075806204515, 'eval_runtime': 3.994, 'eval_samples_per_second': 261.14, 'eval_steps_per_second': 16.525, 'epoch': 9.0}\n",
            "{'loss': 0.4379, 'learning_rate': 2e-05, 'epoch': 9.35}\n",
            "[INFO|trainer.py:566] 2022-12-27 17:31:08,754 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_attention_mask, old_input_ids, idx. If sentence, old_attention_mask, old_input_ids, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2022-12-27 17:31:08,756 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2022-12-27 17:31:08,756 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2022-12-27 17:31:08,756 >>   Batch size = 16\n",
            "12/27/2022 17:31:12 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.5219295620918274, 'eval_matthews_correlation': 0.4063609883970663, 'eval_runtime': 3.9891, 'eval_samples_per_second': 261.46, 'eval_steps_per_second': 16.545, 'epoch': 10.0}\n",
            "{'loss': 0.4356, 'learning_rate': 2e-05, 'epoch': 10.28}\n",
            "[INFO|trainer.py:566] 2022-12-27 17:32:16,153 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, old_attention_mask, old_input_ids, idx. If sentence, old_attention_mask, old_input_ids, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2022-12-27 17:32:16,155 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2022-12-27 17:32:16,155 >>   Num examples = 1043\n",
            "[INFO|trainer.py:2421] 2022-12-27 17:32:16,155 >>   Batch size = 16\n",
            "12/27/2022 17:32:20 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "{'eval_loss': 0.4964247941970825, 'eval_matthews_correlation': 0.44110773079251625, 'eval_runtime': 4.006, 'eval_samples_per_second': 260.358, 'eval_steps_per_second': 16.475, 'epoch': 11.0}\n",
            "{'loss': 0.4436, 'learning_rate': 2e-05, 'epoch': 11.21}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls papa/transformers/papa_scripts/outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGNQVD_Vw9n-",
        "outputId": "0084be9d-47cb-4a03-b7bd-b95a3749d189"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention_patterns_sums.pt  attention_sums.pt  avgs.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.rmtree('papa/transformers/papa_scripts/cache')\n",
        "shutil.rmtree('papa/transformers/papa_scripts/outputs')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snMpd_nysttT",
        "outputId": "7a6c0294-e1e4-401c-f875-469a14f0e6ff"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cache\t run_papa_glue_avgs_creator.py\trun_papa_ner_avgs_creator.py\n",
            "outputs  run_papa_glue.py\t\trun_papa_ner.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "source activate env\n",
        "python\n",
        "\n",
        "import torch\n",
        "attention_sums = torch.load('papa/transformers/papa_scripts/outputs/attention_sums.pt')\n",
        "print(f\"Shape of attention_sums : {attention_sums.size()}\")         # (num_layers=12, num_heads=12, )\n",
        "\n",
        "print(attention_sums[0][0].min())\n",
        "# DIMENSIONS :\n",
        "\n",
        "# num_layers = 12\n",
        "# num_heads = 12\n",
        "# num_input_tokens (n) = 64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KD6Ocg8VwZPv",
        "outputId": "90dd9747-6a82-424c-b9a1-e123c3a98697"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of attention_sums : torch.Size([12, 12, 64, 64])\n",
            "tensor(0.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7aJIfQjrPB9l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}